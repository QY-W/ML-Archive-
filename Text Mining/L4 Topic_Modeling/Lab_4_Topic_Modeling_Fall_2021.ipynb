{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 4. TOPIC MODELING - ANSWERS\n",
    "\n",
    "### **<font color=green>INSTRUCTIONS:</font>** <br> \n",
    "\n",
    "**<font color=green> 1. Look for EXERCISES in the script (3 in total).</font>** <br>\n",
    "\n",
    "**<font color=green> 2. Each student INDIVIDUALLY uploads this script with their answers embedded to Canvas by the end of the lab session or by Wednesday, 11:59pm CT (St. Louis time).</font>** \n",
    "\n",
    "### Lab Objectives\n",
    "\n",
    "1. Learn how to estimate a topic model in Python (using the sklearn package)\n",
    "2. Get familiar with the output of a topic model\n",
    "3. Visualize topics in a text corpus\n",
    "4. Evaluate and discriminate between topic models\n",
    "\n",
    "### Session Prep\n",
    "Below we install the modules we need and define the text normalization function we used in Lab 3, as well as two addtional function we need for today only.\n",
    "\n",
    "**Important:** Make sure Text_Normalization_Function.ipynb file is in the same directory as the current notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: You are using pip version 21.1; however, version 22.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.20.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.2.4)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas) (1.20.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: You are using pip version 21.1; however, version 22.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.6.5)\n",
      "Requirement already satisfied: click in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (2021.11.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (4.62.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: You are using pip version 21.1; however, version 22.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sklearn) (1.0.1)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->sklearn) (1.6.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->sklearn) (3.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->sklearn) (1.20.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: You are using pip version 21.1; however, version 22.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n",
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "ERROR: Invalid requirement: '#visualizing'\n",
      "WARNING: You are using pip version 21.1; however, version 22.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: html.parser in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.2)\n",
      "Requirement already satisfied: ply in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from html.parser) (3.11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: You are using pip version 21.1; however, version 22.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pattern3 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pattern3) (4.10.0)\n",
      "Requirement already satisfied: cherrypy in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pattern3) (18.6.1)\n",
      "Requirement already satisfied: docx in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pattern3) (0.2.4)\n",
      "Requirement already satisfied: feedparser in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pattern3) (6.0.8)\n",
      "Requirement already satisfied: pdfminer3k in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pattern3) (1.3.4)\n",
      "Requirement already satisfied: simplejson in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pattern3) (3.17.5)\n",
      "Requirement already satisfied: pdfminer.six in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pattern3) (20211012)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from beautifulsoup4->pattern3) (2.2.1)\n",
      "Requirement already satisfied: jaraco.collections in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cherrypy->pattern3) (3.4.0)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cherrypy->pattern3) (8.10.0)\n",
      "Requirement already satisfied: pywin32>=227 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cherrypy->pattern3) (300)\n",
      "Requirement already satisfied: portend>=2.1.1 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cherrypy->pattern3) (3.0.0)\n",
      "Requirement already satisfied: zc.lockfile in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cherrypy->pattern3) (2.0)\n",
      "Requirement already satisfied: cheroot>=8.2.1 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cherrypy->pattern3) (8.5.2)\n",
      "Requirement already satisfied: jaraco.functools in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cheroot>=8.2.1->cherrypy->pattern3) (3.4.0)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cheroot>=8.2.1->cherrypy->pattern3) (1.15.0)\n",
      "Requirement already satisfied: tempora>=1.8 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from portend>=2.1.1->cherrypy->pattern3) (4.1.2)\n",
      "Requirement already satisfied: pytz in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern3) (2021.1)\n",
      "Requirement already satisfied: lxml in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from docx->pattern3) (4.6.4)\n",
      "Requirement already satisfied: Pillow>=2.0 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from docx->pattern3) (8.3.2)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from feedparser->pattern3) (1.0.0)\n",
      "Requirement already satisfied: jaraco.classes in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from jaraco.collections->cherrypy->pattern3) (3.2.1)\n",
      "Requirement already satisfied: jaraco.text in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from jaraco.collections->cherrypy->pattern3) (3.6.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from jaraco.text->jaraco.collections->cherrypy->pattern3) (5.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from importlib-resources->jaraco.text->jaraco.collections->cherrypy->pattern3) (3.6.0)\n",
      "Requirement already satisfied: chardet in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pdfminer.six->pattern3) (4.0.0)\n",
      "Requirement already satisfied: cryptography in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pdfminer.six->pattern3) (35.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cryptography->pdfminer.six->pattern3) (1.14.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cffi>=1.12->cryptography->pdfminer.six->pattern3) (2.20)\n",
      "Requirement already satisfied: ply in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pdfminer3k->pattern3) (3.11)\n",
      "Requirement already satisfied: setuptools in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from zc.lockfile->cherrypy->pattern3) (49.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: You are using pip version 21.1; however, version 22.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.3.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis) (1.1.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis) (49.2.1)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis) (1.2.4)\n",
      "Requirement already satisfied: gensim in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis) (4.1.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis) (1.0.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis) (2.11.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis) (1.6.3)\n",
      "Requirement already satisfied: numexpr in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis) (2.7.3)\n",
      "Requirement already satisfied: funcy in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis) (1.16)\n",
      "Requirement already satisfied: future in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: sklearn in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis) (0.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis) (1.20.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gensim->pyLDAvis) (0.29.23)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from jinja2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->pyLDAvis) (3.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: You are using pip version 21.1; however, version 22.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Willi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Willi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Willi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Willi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  ['<', 'p', '>', 'The', 'circus', 'dog', 'in', 'a', 'plissé', 'skirt', 'jumped', 'over', 'Python', 'who', 'was', \"n't\", 'that', 'large', ',', 'just', '3', 'feet', 'long.', '<', '/p', '>']\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  <p>The circus dog in a plissé skirt jumped over Python who was not that large, just 3 feet long.</p>\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  [('<', 'a'), ('p', 'n'), ('>', 'v'), ('the', None), ('circus', 'n'), ('dog', 'n'), ('in', None), ('a', None), ('plissé', 'n'), ('skirt', 'n'), ('jumped', 'v'), ('over', None), ('python', 'n'), ('who', None), ('was', 'v'), (\"n't\", 'r'), ('that', None), ('large', 'a'), (',', None), ('just', 'r'), ('3', None), ('feet', 'n'), ('long.', 'a'), ('<', 'n'), ('/p', 'n'), ('>', 'n')]\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  < p > the circus dog in a plissé skirt jump over python who be n't that large , just 3 foot long. < /p >\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:    p   The circus dog in a plissé skirt jumped over Python who was n t that large   just 3 feet long     p  \n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  < p > The circus dog plissé skirt jumped Python n't large , 3 feet long. < /p >\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  p The circus dog in a plissé skirt jumped over Python who was n't that large just feet long. /p\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  <p>The circus dog in a plisse skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n"
     ]
    }
   ],
   "source": [
    "#the module 'sys' allows istalling module from inside Jupyter\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install numpy\n",
    "import numpy as np\n",
    "\n",
    "!{sys.executable} -m pip install pandas\n",
    "import pandas as pd\n",
    "\n",
    "#Natrual Language ToolKit (NLTK)\n",
    "!{sys.executable} -m pip install nltk\n",
    "import nltk\n",
    "\n",
    "!{sys.executable} -m pip install sklearn\n",
    "from sklearn import metrics\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import  CountVectorizer #bag-of-words vectorizer \n",
    "from sklearn.decomposition import LatentDirichletAllocation #package for LDA\n",
    "\n",
    "# Plotting tools\n",
    "\n",
    "from pprint import pprint\n",
    "!{sys.executable} -m pip install pyLDAvis #visualizing LDA\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#define text normalization function\n",
    "%run ./Text_Normalization_Function.ipynb #defining text normalization function\n",
    "\n",
    "#ignore warnings about future changes in functions as they take too much space\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define two functions that will display the results of fitting a topic model, to be used later:\n",
    "\n",
    "*Note: these functions are not the focus of the lab, therefore we'll not be discussing them, but you are welcome to explore and dig into them later if you prefer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        \n",
    "def get_topic_words(vectorizer, lda_model, n_words):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_words = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_word_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_words.append(keywords.take(top_word_locs).tolist())\n",
    "    return topic_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Data Example\n",
    "We'll start with working on a toy dataset. It will allow us to grasp the full results of a topic model before moving to high-dimensional realistic data.\n",
    "\n",
    "#### Define and Prep Toy Data\n",
    "Let's use the toy corpus on animals and programming similar to one in the lecture. Let's define it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_corpus = [\"The fox jumps over the dog\", \n",
    "              \"The fox is very clever and quick\", \n",
    "              \"The dog is slow and lazy\", \n",
    "              \"The cat is smarter than the fox and the dog but it can never learn Java\", \n",
    "              \"Python is an excellent programming language\", \n",
    "              \"Java and Ruby are other programming languages\", \n",
    "              \"Python and Java are very popular programming languages\", \n",
    "              \"Python programs are smaller than Java programs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's **normalize** our toy_corpus and call the normalized corpus **normalized_toy_corpus**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_toy_corpus = normalize_corpus(toy_corpus) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since for topic modeling we need text data in the **Bag-of-Words** representation, let's **vectorize** our normalized_toy_corpus and call it **bow_toy_corpus**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the bag-of-words vectorizer:\n",
    "bow_vectorizer = CountVectorizer()\n",
    "\n",
    "#vectorize the normalized data:\n",
    "bow_toy_corpus = bow_vectorizer.fit_transform(normalized_toy_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the Bag-of-Words representation of our corpus: **It never hurts to know how you data look like :)** Note absence of stopwords and other differences with the raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>clever</th>\n",
       "      <th>dog</th>\n",
       "      <th>excellent</th>\n",
       "      <th>fox</th>\n",
       "      <th>java</th>\n",
       "      <th>jump</th>\n",
       "      <th>language</th>\n",
       "      <th>lazy</th>\n",
       "      <th>learn</th>\n",
       "      <th>never</th>\n",
       "      <th>popular</th>\n",
       "      <th>program</th>\n",
       "      <th>programming</th>\n",
       "      <th>python</th>\n",
       "      <th>quick</th>\n",
       "      <th>ruby</th>\n",
       "      <th>slow</th>\n",
       "      <th>small</th>\n",
       "      <th>smarter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat  clever  dog  excellent  fox  java  jump  language  lazy  learn  never  \\\n",
       "0    0       0    1          0    1     0     1         0     0      0      0   \n",
       "1    0       1    0          0    1     0     0         0     0      0      0   \n",
       "2    0       0    1          0    0     0     0         0     1      0      0   \n",
       "3    1       0    1          0    1     1     0         0     0      1      1   \n",
       "4    0       0    0          1    0     0     0         1     0      0      0   \n",
       "5    0       0    0          0    0     1     0         1     0      0      0   \n",
       "6    0       0    0          0    0     1     0         1     0      0      0   \n",
       "7    0       0    0          0    0     1     0         0     0      0      0   \n",
       "\n",
       "   popular  program  programming  python  quick  ruby  slow  small  smarter  \n",
       "0        0        0            0       0      0     0     0      0        0  \n",
       "1        0        0            0       0      1     0     0      0        0  \n",
       "2        0        0            0       0      0     0     1      0        0  \n",
       "3        0        0            0       0      0     0     0      0        1  \n",
       "4        0        0            1       1      0     0     0      0        0  \n",
       "5        0        0            1       0      0     1     0      0        0  \n",
       "6        1        0            1       1      0     0     0      0        0  \n",
       "7        0        2            0       1      0     0     0      1        0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data = bow_toy_corpus.todense(), columns = bow_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Model (via Latent Dirichlet Allocation) on Toy Data\n",
    "Now let's **model topics** in our toy data. Given that the toy corpus is so small, we know all \"topics\" it contains (**what are they?**) and it will be easy for us:<br> 1) to check if the topic model results make sense; <br>2) see all the results that the topic model produces.  <br><br>\n",
    "We will be using the **LatentDirichletAllocation** function which we already imported earlier (see Session Prep). The function has the following **parameters** to be set:\n",
    "1. Number of topics to model: **n_components**\n",
    "2. Parameter vector for the Dirichlet distribution for *topics*: **doc_topic_prior**\n",
    "3. Parameter vector for the Dirichlet distribution for *words* in a topic: **topic_word_prior**\n",
    "\n",
    "Notes on **parameter vectors for the Dirichlet distributions**: <br>\n",
    "1. Although the Dirichlet distribution parameters are represented by a **vector**, for simplicity we provide one number for each parameter vector. For example, if we set the number of topics to 2 (n_components=2), the parameter vector for the Dirichlet distribution for *topics* should be a two-dimensional vector. We set doc_topic_prior=0.5 and the LatentDirichletAllocation function internally creates a two-dimensional vector (0.5,0.5). Similar logic applies to the parameter vector for the Dirichlet distribution for *words*.<br><br>\n",
    "2. Remember, that we need **sparsity** in the distribution of topics across documents (i.e., some documents have a zero probability of containing some of the topics) and *sparsity* in the distribution of words in topics (i.e., some words have zero probability to be present in some topics). To induce sparsity, we need to set doc_topic_prior and doc_topic_prior between 0 and 1.\n",
    "\n",
    "Now, let's set the parameters and estimate the topic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_toy_corpus = LatentDirichletAllocation(n_components=2, max_iter=500,\n",
    "                                           doc_topic_prior = 0.9,\n",
    "                                           topic_word_prior = 0.9).fit(bow_toy_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display results by showing 15 **most frequent (top)** words for each topic (we use **function display_topics** defined in Session Prep):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "java python language programming program popular small excellent ruby dog fox lazy slow clever quick\n",
      "Topic 1:\n",
      "fox dog smarter never learn cat jump quick clever lazy slow java programming language python\n"
     ]
    }
   ],
   "source": [
    "no_top_words = 15\n",
    "display_topics(lda_toy_corpus, bow_vectorizer.get_feature_names(), no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that topics do not have names or labels. **Topics are just collections of words**, following the definition of a topic in text mining. <br><br> To be precise, topics are **word vectors**, where each vector element is the **weight** (relative frequency) of the word in a topic. Let's have a look at those \"word vectors\". Can you see below that each word vector (topic) is a **simplex**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>clever</th>\n",
       "      <th>dog</th>\n",
       "      <th>excellent</th>\n",
       "      <th>fox</th>\n",
       "      <th>java</th>\n",
       "      <th>jump</th>\n",
       "      <th>language</th>\n",
       "      <th>lazy</th>\n",
       "      <th>learn</th>\n",
       "      <th>never</th>\n",
       "      <th>popular</th>\n",
       "      <th>program</th>\n",
       "      <th>programming</th>\n",
       "      <th>python</th>\n",
       "      <th>quick</th>\n",
       "      <th>ruby</th>\n",
       "      <th>slow</th>\n",
       "      <th>small</th>\n",
       "      <th>smarter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.025956</td>\n",
       "      <td>0.026564</td>\n",
       "      <td>0.026766</td>\n",
       "      <td>0.050982</td>\n",
       "      <td>0.026766</td>\n",
       "      <td>0.109886</td>\n",
       "      <td>0.026407</td>\n",
       "      <td>0.105756</td>\n",
       "      <td>0.026564</td>\n",
       "      <td>0.025956</td>\n",
       "      <td>0.025956</td>\n",
       "      <td>0.051208</td>\n",
       "      <td>0.078480</td>\n",
       "      <td>0.105756</td>\n",
       "      <td>0.105892</td>\n",
       "      <td>0.026564</td>\n",
       "      <td>0.050859</td>\n",
       "      <td>0.026564</td>\n",
       "      <td>0.051163</td>\n",
       "      <td>0.025956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.055089</td>\n",
       "      <td>0.054435</td>\n",
       "      <td>0.113485</td>\n",
       "      <td>0.028202</td>\n",
       "      <td>0.113486</td>\n",
       "      <td>0.053819</td>\n",
       "      <td>0.054603</td>\n",
       "      <td>0.028622</td>\n",
       "      <td>0.054435</td>\n",
       "      <td>0.055089</td>\n",
       "      <td>0.055089</td>\n",
       "      <td>0.027959</td>\n",
       "      <td>0.028292</td>\n",
       "      <td>0.028622</td>\n",
       "      <td>0.028476</td>\n",
       "      <td>0.054435</td>\n",
       "      <td>0.028333</td>\n",
       "      <td>0.054435</td>\n",
       "      <td>0.028007</td>\n",
       "      <td>0.055089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cat    clever       dog  excellent       fox      java      jump  \\\n",
       "0  0.025956  0.026564  0.026766   0.050982  0.026766  0.109886  0.026407   \n",
       "1  0.055089  0.054435  0.113485   0.028202  0.113486  0.053819  0.054603   \n",
       "\n",
       "   language      lazy     learn     never   popular   program  programming  \\\n",
       "0  0.105756  0.026564  0.025956  0.025956  0.051208  0.078480     0.105756   \n",
       "1  0.028622  0.054435  0.055089  0.055089  0.027959  0.028292     0.028622   \n",
       "\n",
       "     python     quick      ruby      slow     small   smarter  \n",
       "0  0.105892  0.026564  0.050859  0.026564  0.051163  0.025956  \n",
       "1  0.028476  0.054435  0.028333  0.054435  0.028007  0.055089  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_weights = lda_toy_corpus.components_ / lda_toy_corpus.components_.sum(axis=1)[:, np.newaxis]\n",
    "pd.DataFrame(word_weights.T, index = bow_vectorizer.get_feature_names()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font color=green>EXERCISE 1:</font>**\n",
    "\n",
    "**<font color=green>1.1. Adjust doc_topic_prior (alpha) and topic_word_prior (beta) and observe the changes in topic representation.</font>** \n",
    "\n",
    "**<font color=green>1.2. You are likely to be not 100% satisfied with the model performance, even after all the adjustments. The word \"java\" might still be appearing among the top words in the \"animals\" topic. Why? Looking at the text corpus might help to find the answer.</font>**\n",
    "\n",
    "**Answer 1.1:** <br>\n",
    "\n",
    "Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "java python language programming program popular small excellent ruby lazy slow clever quick jump dog\n",
      "Topic 1:\n",
      "fox dog smarter never learn cat jump quick clever lazy slow java ruby excellent programming\n"
     ]
    }
   ],
   "source": [
    "lda_toy_corpus = LatentDirichletAllocation(n_components=2, max_iter=500,\n",
    "                                           doc_topic_prior = 0.5,\n",
    "                                           topic_word_prior = 0.5).fit(bow_toy_corpus)\n",
    "no_top_words = 15\n",
    "display_topics(lda_toy_corpus, bow_vectorizer.get_feature_names(), no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion: \n",
    "adjust alpha, beta in about cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 1.2:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:Java is a island. And there can be text talking about animal living on java island."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling on Real Data\n",
    "\n",
    "The dataset here is the one we used for doing Text Classification in Lab 3. The newspaper blog posts have 4 topics: **atheism, religion, computer graphics, and space science**. Of course, we will *not use* class label information for topic modeling.\n",
    "\n",
    "Download the data and set up the data (**news_corpus**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "dataset = fetch_20newsgroups(shuffle=True, \n",
    "                             random_state=1, \n",
    "                             categories = categories, \n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "news_corpus = dataset.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's normalize the corpus and create the Bag-of-Words representation of the data. We'll limit the number of features to **1000 most frequent features** to compute the topic model faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize data\n",
    "normalized_corpus_news = normalize_corpus(news_corpus)\n",
    "\n",
    "#define a Bag-of-Words vecgtorizer\n",
    "bow_vectorizer_news = CountVectorizer(max_features=1000)\n",
    "\n",
    "#vectorize data\n",
    "bow_news_corpus = bow_vectorizer_news.fit_transform(normalized_corpus_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit the topic model. We need to **set the number of topics** first. We are *lucky to know* that there are **4 topics** (atheism, religion, computer graphics, and space science) and it will allow us to judge the performance of the topic model better.\n",
    "\n",
    "**Note**: It will take a couple of minutes for the estimation to finish. The larger the number of iterations (max_iter) you allow for, the longer it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_news = LatentDirichletAllocation(n_components=4, max_iter=100,\n",
    "                                     doc_topic_prior = 0.25,\n",
    "                                     topic_word_prior = 0.25).fit(bow_news_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display results with top 10 words for each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "think like know could time people good point thing take\n",
      "Topic 1:\n",
      "god people jesus christian believe religion bible think many atheist\n",
      "Topic 2:\n",
      "space nasa launch satellite system orbit mission earth year use\n",
      "Topic 3:\n",
      "image file use edu program graphic software format jpeg ftp\n"
     ]
    }
   ],
   "source": [
    "no_top_words_news = 10\n",
    "display_topics(lda_news, bow_vectorizer_news.get_feature_names(), no_top_words_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display **word vectors** (words are in alphabetical order) for each topic. Each column is a topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_0</th>\n",
       "      <th>Topic_1</th>\n",
       "      <th>Topic_2</th>\n",
       "      <th>Topic_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3d</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.005196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>able</th>\n",
       "      <td>0.002125</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.001327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ac</th>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.001251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accept</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.003642</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acceptable</th>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.000192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>access</th>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>0.001923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accord</th>\n",
       "      <td>0.000545</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>account</th>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>act</th>\n",
       "      <td>0.000892</td>\n",
       "      <td>0.002197</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>action</th>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.002172</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Topic_0   Topic_1   Topic_2   Topic_3\n",
       "3d          0.000008  0.000008  0.000010  0.005196\n",
       "able        0.002125  0.000353  0.000298  0.001327\n",
       "ac          0.000067  0.000008  0.000391  0.001251\n",
       "accept      0.000008  0.003642  0.000011  0.000491\n",
       "acceptable  0.000187  0.000687  0.000320  0.000192\n",
       "access      0.000322  0.000008  0.000920  0.001923\n",
       "accord      0.000545  0.001621  0.000390  0.000009\n",
       "account     0.000838  0.000479  0.000078  0.000051\n",
       "act         0.000892  0.002197  0.000828  0.000010\n",
       "action      0.000553  0.002172  0.000327  0.000010"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_weights = lda_news.components_ / lda_news.components_.sum(axis=1)[:, np.newaxis]\n",
    "word_weights_df = pd.DataFrame(word_weights.T, \n",
    "                               index = bow_vectorizer_news.get_feature_names(), \n",
    "                               columns = [\"Topic_\" + str(i) for i in range(4)])\n",
    "word_weights_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, **sort by word weights in Topic 0** (descending order) and see the weights by 10 most frequent words in Topic 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_0</th>\n",
       "      <th>Topic_1</th>\n",
       "      <th>Topic_2</th>\n",
       "      <th>Topic_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>think</th>\n",
       "      <td>0.015079</td>\n",
       "      <td>0.008062</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0.014474</td>\n",
       "      <td>0.003435</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.004175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>know</th>\n",
       "      <td>0.013985</td>\n",
       "      <td>0.006530</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.005439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>could</th>\n",
       "      <td>0.011021</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>0.001809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>0.009993</td>\n",
       "      <td>0.003761</td>\n",
       "      <td>0.005238</td>\n",
       "      <td>0.003081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>0.009966</td>\n",
       "      <td>0.013211</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.001214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>0.009369</td>\n",
       "      <td>0.004991</td>\n",
       "      <td>0.001552</td>\n",
       "      <td>0.003725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>point</th>\n",
       "      <td>0.008988</td>\n",
       "      <td>0.004331</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.002330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thing</th>\n",
       "      <td>0.008671</td>\n",
       "      <td>0.005218</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.001256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>take</th>\n",
       "      <td>0.008028</td>\n",
       "      <td>0.005156</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>0.000727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Topic_0   Topic_1   Topic_2   Topic_3\n",
       "think   0.015079  0.008062  0.000117  0.000335\n",
       "like    0.014474  0.003435  0.000052  0.004175\n",
       "know    0.013985  0.006530  0.000323  0.005439\n",
       "could   0.011021  0.003057  0.000981  0.001809\n",
       "time    0.009993  0.003761  0.005238  0.003081\n",
       "people  0.009966  0.013211  0.000239  0.001214\n",
       "good    0.009369  0.004991  0.001552  0.003725\n",
       "point   0.008988  0.004331  0.000012  0.002330\n",
       "thing   0.008671  0.005218  0.000326  0.001256\n",
       "take    0.008028  0.005156  0.001599  0.000727"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_weights_df.sort_values(by='Topic_0',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Model Visualization\n",
    "\n",
    "You can **visualize** the topics: topic size, frequency of words in a topic and so on.\n",
    "\n",
    "In this visualization, you can rank words in a topic by **relevancy**: do you want rare and exclusive terms (i.e. found mostly in that topic) or terms that are used frequently in that topic, but not always exclusive to that topic? Relevancy parameter is λ (0 ≤ λ ≤ 1). You can adjust it:\n",
    "\n",
    "* small λ highlights potentially rare, but exclusive terms for the selected topic;\n",
    "* large values of λ (near 1) highlight frequent, but not necessarily exclusive, terms for the selected topic;\n",
    "\n",
    "Relevancy is measured as: \n",
    "\n",
    "    Relevancy = λ log[p(term | topic)] + (1 - λ) log[p(term | topic)/p(term)], \n",
    "   \n",
    "   where p(term | topic) stands for word weight in a topic and p(term) stands for word's weight in a corpus.\n",
    "\n",
    "Additional information on how to use this visualization:\n",
    "* http://www.kennyshirley.com/LDAvis/\n",
    "* https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf\n",
    "\n",
    "We installed all the **pyLDAvis** module required for this visualization in Session Prep. Now let's use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el3738818898202018082505376210\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el3738818898202018082505376210_data = {\"mdsDat\": {\"x\": [269.5624084472656, 1.782994270324707, 220.81143188476562, 50.533992767333984], \"y\": [-124.80339813232422, -295.0796813964844, -343.8321228027344, -76.05093383789062], \"topics\": [1, 2, 3, 4], \"cluster\": [1, 1, 1, 1], \"Freq\": [28.19416196254092, 26.968686306465962, 23.756903903830743, 21.08024782716238]}, \"tinfo\": {\"Term\": [\"space\", \"image\", \"god\", \"file\", \"nasa\", \"launch\", \"edu\", \"jesus\", \"christian\", \"satellite\", \"program\", \"think\", \"format\", \"graphic\", \"orbit\", \"religion\", \"jpeg\", \"software\", \"ftp\", \"mission\", \"bible\", \"data\", \"color\", \"atheist\", \"believe\", \"people\", \"mail\", \"shuttle\", \"argument\", \"year\", \"jesus\", \"christian\", \"god\", \"religion\", \"bible\", \"atheist\", \"argument\", \"belief\", \"law\", \"atheism\", \"moral\", \"religious\", \"faith\", \"morality\", \"islam\", \"christ\", \"existence\", \"muslim\", \"christianity\", \"interpretation\", \"gay\", \"fallacy\", \"false\", \"mormon\", \"jew\", \"islamic\", \"argue\", \"homosexual\", \"sex\", \"premise\", \"truth\", \"conclusion\", \"ra\", \"statement\", \"believe\", \"evidence\", \"context\", \"church\", \"accept\", \"exist\", \"true\", \"people\", \"objective\", \"person\", \"many\", \"claim\", \"must\", \"life\", \"mean\", \"example\", \"human\", \"think\", \"even\", \"may\", \"word\", \"know\", \"way\", \"use\", \"well\", \"thing\", \"take\", \"koresh\", \"juda\", \"tyre\", \"prophecy\", \"prophet\", \"night\", \"quran\", \"p2\", \"den\", \"p3\", \"p1\", \"hang\", \"gun\", \"fbi\", \"greek\", \"saw\", \"guy\", \"tax\", \"dead\", \"master\", \"eat\", \"maybe\", \"matthew\", \"normal\", \"stay\", \"king\", \"mary\", \"burn\", \"buy\", \"face\", \"enough\", \"big\", \"little\", \"plane\", \"like\", \"could\", \"think\", \"hear\", \"long\", \"sure\", \"problem\", \"really\", \"know\", \"probably\", \"point\", \"thing\", \"never\", \"happen\", \"right\", \"lot\", \"give\", \"time\", \"good\", \"take\", \"much\", \"actually\", \"still\", \"day\", \"want\", \"people\", \"year\", \"well\", \"look\", \"even\", \"way\", \"seem\", \"us\", \"since\", \"use\", \"might\", \"first\", \"format\", \"jpeg\", \"ftp\", \"gif\", \"pub\", \"3d\", \"file\", \"graphics\", \"email\", \"directory\", \"pc\", \"mac\", \"screen\", \"unix\", \"driver\", \"pixel\", \"sgi\", \"comp\", \"amiga\", \"polygon\", \"viewer\", \"tiff\", \"internet\", \"animation\", \"output\", \"postscript\", \"tar\", \"input\", \"shareware\", \"visualization\", \"graphic\", \"color\", \"display\", \"image\", \"edu\", \"software\", \"server\", \"mail\", \"package\", \"thanks\", \"window\", \"available\", \"code\", \"program\", \"version\", \"computer\", \"user\", \"use\", \"send\", \"data\", \"bit\", \"system\", \"information\", \"please\", \"write\", \"include\", \"line\", \"know\", \"nasa\", \"launch\", \"satellite\", \"orbit\", \"shuttle\", \"lunar\", \"rocket\", \"flight\", \"station\", \"national\", \"probe\", \"spacecraft\", \"solar\", \"dc\", \"vehicle\", \"planetary\", \"orbital\", \"payload\", \"telescope\", \"orbiter\", \"propulsion\", \"astronaut\", \"venus\", \"km\", \"radio\", \"atmosphere\", \"fuel\", \"titan\", \"saturn\", \"jupiter\", \"space\", \"mission\", \"development\", \"technology\", \"fund\", \"moon\", \"mar\", \"sci\", \"earth\", \"report\", \"service\", \"center\", \"design\", \"research\", \"cost\", \"year\", \"system\", \"science\", \"data\", \"program\", \"use\", \"first\", \"new\", \"include\", \"time\"], \"Freq\": [1043.0, 846.0, 785.0, 546.0, 413.0, 399.0, 435.0, 408.0, 366.0, 284.0, 532.0, 751.0, 281.0, 292.0, 253.0, 301.0, 264.0, 293.0, 255.0, 242.0, 285.0, 443.0, 246.0, 256.0, 413.0, 787.0, 241.0, 201.0, 248.0, 440.0, 408.07648726812397, 365.4036768502851, 784.1147690348366, 300.90982743189284, 285.0307449367275, 255.26770660639698, 247.3050585475291, 206.64355352345925, 212.57111570205612, 198.69577257522815, 171.9109603594202, 157.02777666283507, 138.172624502393, 122.30056421762343, 120.31335264766112, 98.48109518222331, 92.51394127753625, 86.56961453830235, 85.57778057896415, 85.55146200139217, 72.67870913724768, 69.70967106535839, 68.7080334225767, 67.72039782053507, 67.71017766620918, 63.74339745117522, 65.69195211134978, 61.76691578400967, 61.76205514317842, 59.78189383164816, 117.09473643101198, 109.17980022771658, 94.32750580158634, 131.5617245143298, 359.96535689291403, 178.34276242929144, 99.40473216603509, 104.47930991286336, 119.81841271338214, 196.1867426077085, 208.47812500744095, 434.5923978513993, 118.36690606939194, 138.82074974418998, 256.992461982726, 161.86119773542535, 181.60749258541955, 180.80648686926068, 203.92465193416135, 169.63252609350448, 143.06299325800876, 265.21422404116197, 209.2996842726105, 199.16670506857312, 160.28790857751468, 214.79391252633002, 176.81083350147205, 200.3316107961656, 173.55500133173342, 171.64882066659646, 169.62186862274905, 75.12251588690087, 59.36573126322739, 58.381860396923194, 59.35383196592208, 58.36002902646533, 57.370762873208335, 65.13456653583228, 52.47373544307637, 51.48855042312143, 50.50312256137419, 47.54549282923606, 47.52704564758906, 45.55659383078897, 39.651917222420415, 63.29631398388323, 66.0586955684043, 56.98097175999976, 42.48825623142814, 52.280352590289745, 41.411987449230814, 38.46610847647469, 109.33942196304268, 111.93385678684578, 37.29195518575299, 47.84113700654557, 51.709916797226406, 37.494126932705775, 40.81657761878481, 64.94737245561731, 93.36132665921387, 130.7613530389891, 124.71815334989167, 156.40867799825367, 76.15595083052315, 455.42478493080796, 346.7832893393628, 474.4677713997524, 118.27452811586039, 173.18340749747472, 136.47567377469878, 210.32851337148165, 177.6629572157955, 440.0412921481151, 126.71571385432938, 282.82650061851797, 272.8362684045604, 132.68553845056695, 91.2068153412269, 180.79372269518782, 133.1934844311251, 235.4951409729721, 314.4501537301283, 294.8117618201905, 252.60696910923323, 220.22011551882255, 129.65045946467467, 153.99800431545805, 161.91878528762462, 200.6481036198166, 313.5758521677421, 217.9051442706461, 238.85220938079127, 208.22018672134652, 211.82469971773702, 202.72452877642, 171.60932554043276, 170.082002910271, 150.60497721156239, 217.12209570751287, 148.57028463028288, 149.4344930184183, 280.87785803592635, 264.0190152416652, 255.0890050213761, 196.5839863937679, 196.5788322739341, 144.03068960376706, 543.5641970288715, 143.04164622106512, 141.00273915893666, 121.22238968018414, 113.2762183596415, 113.27421835854511, 92.45195799497421, 88.49379122105782, 81.53215640393118, 79.55767638557373, 75.59963111771235, 73.62380938898006, 73.62215019240465, 71.63561718578462, 70.64686331762236, 68.66289225637749, 66.66160871263315, 64.70011430956106, 57.750067670348685, 56.76543496062674, 55.77791502790615, 55.739105921595176, 51.811189075056774, 51.80908429951799, 287.81974668993337, 241.43810752158925, 180.92479433583847, 800.9175668028918, 416.6723790621534, 282.4958996911897, 109.75793511399145, 233.0804962186881, 178.44150220316644, 172.26915369517164, 100.26853432421188, 249.13417722337806, 157.57691688772857, 353.01601300239787, 189.5098704874365, 155.66583844234685, 139.49592523879122, 459.63592939413275, 182.9327093390085, 250.6326556555308, 183.51276935544468, 231.84362782116642, 168.48074485062025, 161.89253913600282, 153.8333800799857, 151.96551545780696, 147.0983567616888, 150.75431072002718, 412.44398299754204, 398.4859677933001, 283.70457049355457, 252.76269413086357, 200.8566746748383, 181.8994182266472, 153.951515050617, 135.98105688572835, 129.97501353849125, 128.96746234276935, 123.0088395401218, 115.02596171706215, 110.03335827699293, 110.03278548727481, 107.03552559522274, 88.05498392835948, 77.09381768195948, 72.10395163516331, 69.09055462199754, 65.12179255904468, 63.122368741174014, 62.12542005718301, 61.12188583629241, 59.129053112245664, 59.11151442211757, 55.118708704513814, 53.13857007436358, 51.14819570936962, 51.14769278531266, 50.13948966110698, 1022.0631227585241, 237.94356550492623, 115.93738184772579, 159.24680679324317, 84.92446622831947, 165.5248636663477, 104.4527979358841, 94.94269998384166, 230.78438668647635, 117.42990707259635, 103.86737378389869, 162.71094204964584, 129.72010941009674, 135.83088548817258, 136.60777709188497, 214.12130579912815, 259.07022375761045, 145.62592680017306, 189.82907341106073, 179.15133192545036, 210.87674834241176, 151.14025675047785, 136.22045867842547, 131.86652639997706, 128.82995646248773], \"Total\": [1043.0, 846.0, 785.0, 546.0, 413.0, 399.0, 435.0, 408.0, 366.0, 284.0, 532.0, 751.0, 281.0, 292.0, 253.0, 301.0, 264.0, 293.0, 255.0, 242.0, 285.0, 443.0, 246.0, 256.0, 413.0, 787.0, 241.0, 201.0, 248.0, 440.0, 408.8360560099246, 366.1662786876735, 785.9177263219972, 301.6653376526496, 285.7881656314193, 256.01856533731, 248.07995714178207, 207.39480403003475, 213.3487742711138, 199.45621642693138, 172.6635209743953, 157.77869395318137, 138.92456951548752, 123.04744824518139, 121.06278670581466, 99.23169713803111, 93.27774083766656, 87.32380973450954, 86.33149831721802, 86.33155666577495, 73.43133624705251, 70.45440191182573, 69.46201909098454, 68.46973195628844, 68.46965631936797, 64.50039791678812, 66.48500268157872, 62.51580141699807, 62.515771474417434, 60.53115174384219, 119.08532144999441, 111.1465984032144, 96.26143119652797, 135.94290972258648, 413.6748634029524, 196.35884482273173, 104.19425875889041, 113.06941237442948, 133.95383488876857, 244.20608694992845, 281.4779975620047, 787.705181427122, 135.0477275323633, 169.68529829711096, 477.77369641199533, 227.8482332530207, 291.46122904890757, 292.18734806562844, 363.4372927497562, 265.8711147017622, 193.47097549886544, 751.84747400329, 455.95611178795775, 467.3496140732687, 275.51779381015746, 813.5448163074562, 433.3169016659558, 1087.9663842402229, 519.4788104608459, 487.32836291642354, 481.7076078923235, 75.88443437006329, 60.11753107665511, 59.13209645479355, 60.11761651405796, 59.13224729407884, 58.146930053074136, 66.03110170541468, 53.21950097584758, 52.23407258686856, 51.24864573693697, 48.29237492853731, 48.2925106498688, 46.32171833581399, 40.40903814882845, 65.05097389544484, 68.01619143653001, 59.141012926497204, 44.35783511778298, 55.21799012307885, 44.36465648981899, 41.40968208797475, 120.29820558742773, 124.25066188488128, 41.415710152762955, 53.26021988205203, 58.18652091428047, 42.43305029728275, 46.369890286735824, 73.965460582158, 108.5591494560992, 155.90792711606062, 149.95456198319945, 194.4595071368451, 89.84455089448305, 685.412894829473, 521.6251787132345, 751.84747400329, 150.99183727911688, 240.19481316471308, 181.6181268349263, 312.17701462256923, 253.7837835707517, 813.5448163074562, 170.79440941927743, 490.17015617786205, 487.32836291642354, 186.6199332047396, 113.48019519463499, 290.50021128553544, 191.60582874853304, 427.13285445933974, 652.4287838044493, 600.4273129131259, 481.7076078923235, 401.4717861981976, 189.61026177669387, 246.13907967400866, 266.10013339591785, 373.65775628127875, 787.705181427122, 440.3082436092919, 519.4788104608459, 421.3304122258967, 455.95611178795775, 433.3169016659558, 324.2641708509286, 362.26709214274797, 274.93679608954324, 1087.9663842402229, 272.94123482926517, 411.6229809866751, 281.626730844798, 264.7688114977501, 255.84407241787395, 197.33713398902924, 197.33717675371454, 144.78003020734397, 546.3966893890498, 143.7883993652841, 141.8051942329988, 121.97228856470346, 114.03921983399827, 114.03911844394138, 93.21460463657408, 89.24810813648105, 82.30659797863837, 80.32339273997209, 76.35667298989483, 74.37341185686628, 74.37344473925614, 72.39007950617044, 71.39851794612524, 69.41516451982366, 67.43205862965416, 65.448632896843, 58.50716483496162, 57.51547445075018, 56.52386617815727, 56.52407048477125, 52.55728877839507, 52.55730672900647, 292.509966243043, 246.94995267500894, 185.45534415286195, 846.1195578718308, 435.3671852620636, 293.5871550341076, 112.06584232718146, 241.96661212598767, 184.4800265956296, 185.47053179646667, 106.09164732203672, 315.7694606835796, 191.40983615408084, 532.6801821305861, 248.79250085048795, 192.48656267498382, 170.7515824829809, 1087.9663842402229, 263.95575200716524, 443.5033397874215, 284.12978288511147, 634.4436520351212, 294.1892674007484, 291.1974900302042, 345.75356361701887, 381.4243222029962, 274.10021002935355, 813.5448163074562, 413.21202605415317, 399.2385647742084, 284.4556782783618, 253.51417252738247, 201.61232274145613, 182.64823414255483, 154.70107522275302, 136.73502210132506, 130.74620156578607, 129.74806696852139, 123.7596049603844, 115.77470910252121, 110.78413294153114, 110.78412103663486, 107.78974627745134, 88.82550990885238, 77.84642015916081, 72.85586472399692, 69.86141952993977, 65.86911659772659, 63.87288205355685, 62.87476155187655, 61.87659598166255, 59.8803937183162, 59.88025325590624, 55.887803728704874, 53.891691365401186, 51.89554180321951, 51.89554524970474, 50.89736214975846, 1043.8816100973786, 242.51262824288912, 119.74717342812485, 173.57660819508817, 88.7856173567953, 183.4255776912123, 111.7283506297655, 101.76151238854835, 280.00238532725706, 129.6810520731428, 117.68695557282636, 213.09608387333273, 177.35795915846575, 192.1189030567674, 198.95123247976397, 440.3082436092919, 634.4436520351212, 238.00776090956273, 443.5033397874215, 532.6801821305861, 1087.9663842402229, 411.6229809866751, 376.91426256390696, 381.4243222029962, 652.4287838044493], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.3896, -4.5001, -3.7365, -4.6943, -4.7485, -4.8588, -4.8905, -5.0701, -5.0418, -5.1093, -5.2541, -5.3447, -5.4726, -5.5946, -5.611, -5.8112, -5.8737, -5.9402, -5.9517, -5.952, -6.1151, -6.1568, -6.1712, -6.1857, -6.1859, -6.2462, -6.2161, -6.2777, -6.2778, -6.3104, -5.6381, -5.7081, -5.8543, -5.5216, -4.5151, -5.2174, -5.8019, -5.7521, -5.6151, -5.122, -5.0613, -4.3267, -5.6273, -5.4679, -4.8521, -5.3144, -5.1993, -5.2037, -5.0834, -5.2675, -5.4378, -4.8206, -5.0573, -5.107, -5.3241, -5.0314, -5.226, -5.1011, -5.2446, -5.2557, -5.2675, -6.0375, -6.2729, -6.2897, -6.2731, -6.29, -6.3071, -6.1802, -6.3963, -6.4153, -6.4346, -6.495, -6.4954, -6.5377, -6.6765, -6.2088, -6.1661, -6.3139, -6.6074, -6.4, -6.6331, -6.7069, -5.6622, -5.6388, -6.7379, -6.4888, -6.411, -6.7325, -6.6476, -6.1831, -5.8202, -5.4833, -5.5306, -5.3042, -6.0239, -4.2354, -4.508, -4.1945, -5.5837, -5.2023, -5.4405, -5.008, -5.1768, -4.2698, -5.5147, -4.7118, -4.7478, -5.4687, -5.8435, -5.1593, -5.4649, -4.895, -4.6058, -4.6703, -4.8248, -4.962, -5.4918, -5.3197, -5.2696, -5.0551, -4.6086, -4.9726, -4.8808, -5.0181, -5.0009, -5.0448, -5.2114, -5.2204, -5.342, -4.9762, -5.3556, -5.3498, -4.5919, -4.6538, -4.6882, -4.9488, -4.9488, -5.2598, -3.9317, -5.2667, -5.2811, -5.4322, -5.5, -5.5, -5.7032, -5.7469, -5.8289, -5.8534, -5.9044, -5.9309, -5.9309, -5.9583, -5.9722, -6.0007, -6.0302, -6.0601, -6.1737, -6.1909, -6.2085, -6.2092, -6.2823, -6.2823, -4.5675, -4.7432, -5.0318, -3.5441, -4.1976, -4.5862, -5.5316, -4.7785, -5.0456, -5.0808, -5.622, -4.7119, -5.1699, -4.3633, -4.9854, -5.1821, -5.2918, -4.0994, -5.0207, -4.7059, -5.0176, -4.7838, -5.103, -5.1429, -5.194, -5.2062, -5.2388, -5.2142, -4.0882, -4.1227, -4.4624, -4.5779, -4.8077, -4.9069, -5.0737, -5.1978, -5.243, -5.2508, -5.2981, -5.3652, -5.4095, -5.4095, -5.4372, -5.6324, -5.7653, -5.8322, -5.8749, -5.9341, -5.9652, -5.9812, -5.9975, -6.0306, -6.0309, -6.1008, -6.1374, -6.1756, -6.1756, -6.1955, -3.1807, -4.6383, -5.3573, -5.0399, -5.6686, -5.0012, -5.4616, -5.557, -4.6688, -5.3445, -5.4672, -5.0183, -5.2449, -5.1989, -5.1932, -4.7438, -4.5532, -5.1293, -4.8642, -4.9221, -4.759, -5.0921, -5.196, -5.2285, -5.2518], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.2642, 1.264, 1.2638, 1.2635, 1.2634, 1.2631, 1.2629, 1.2624, 1.2624, 1.2622, 1.2617, 1.2613, 1.2606, 1.26, 1.2598, 1.2585, 1.2578, 1.2574, 1.2573, 1.257, 1.2558, 1.2554, 1.2551, 1.2551, 1.2549, 1.2542, 1.2541, 1.254, 1.2539, 1.2536, 1.2492, 1.2482, 1.2458, 1.2333, 1.127, 1.1698, 1.219, 1.187, 1.1545, 1.0471, 0.9658, 0.6713, 1.1342, 1.0653, 0.646, 0.9241, 0.793, 0.7861, 0.6882, 0.8167, 0.9642, 0.2241, 0.4874, 0.4131, 0.7244, -0.0657, 0.3697, -0.426, 0.1697, 0.2226, 0.2223, 1.3004, 1.2979, 1.2977, 1.2977, 1.2973, 1.2971, 1.2968, 1.2964, 1.2961, 1.2958, 1.2949, 1.2945, 1.2938, 1.2916, 1.2831, 1.2813, 1.2733, 1.2674, 1.2558, 1.2416, 1.2368, 1.215, 1.2061, 1.2056, 1.2032, 1.1925, 1.1868, 1.1829, 1.1805, 1.1597, 1.1346, 1.1262, 1.0927, 1.1452, 0.9017, 0.9022, 0.8502, 1.0663, 0.9834, 1.0247, 0.9156, 0.9539, 0.696, 1.012, 0.7606, 0.7304, 0.9694, 1.092, 0.8362, 0.9469, 0.7151, 0.5806, 0.5992, 0.665, 0.71, 0.9304, 0.8415, 0.8137, 0.6887, 0.3894, 0.6071, 0.5335, 0.6057, 0.5439, 0.5509, 0.6742, 0.5544, 0.7086, -0.3011, 0.7023, 0.2972, 1.4346, 1.4345, 1.4343, 1.4335, 1.4334, 1.4321, 1.4321, 1.4321, 1.4316, 1.4311, 1.4306, 1.4306, 1.4291, 1.4288, 1.4278, 1.4277, 1.4273, 1.4272, 1.4271, 1.4268, 1.4267, 1.4264, 1.4258, 1.4258, 1.4243, 1.4242, 1.424, 1.4233, 1.423, 1.423, 1.4211, 1.4147, 1.4126, 1.3824, 1.3934, 1.3988, 1.4165, 1.3999, 1.404, 1.3635, 1.3808, 1.2003, 1.2428, 1.0259, 1.1651, 1.225, 1.2351, 0.5757, 1.0706, 0.8666, 1.0002, 0.4306, 0.8799, 0.8502, 0.6274, 0.517, 0.8149, -0.2485, 1.555, 1.5549, 1.5542, 1.5539, 1.5531, 1.5527, 1.552, 1.5513, 1.5509, 1.5508, 1.5507, 1.5503, 1.55, 1.55, 1.5498, 1.5481, 1.5471, 1.5465, 1.5457, 1.5454, 1.545, 1.5448, 1.5446, 1.5442, 1.5439, 1.543, 1.5428, 1.5423, 1.5423, 1.5418, 1.5357, 1.5378, 1.5245, 1.4707, 1.5124, 1.4541, 1.4895, 1.4875, 1.3635, 1.4576, 1.4319, 1.2871, 1.244, 1.2101, 1.1809, 0.8359, 0.6612, 1.0656, 0.7083, 0.4671, -0.084, 0.5549, 0.5391, 0.4947, -0.0654]}, \"token.table\": {\"Topic\": [3, 1, 3, 1, 2, 3, 3, 3, 1, 1, 4, 1, 1, 4, 3, 4, 1, 1, 2, 4, 1, 1, 2, 3, 1, 2, 3, 1, 2, 4, 2, 3, 2, 3, 4, 1, 1, 1, 1, 2, 1, 2, 3, 4, 1, 3, 3, 4, 3, 1, 2, 3, 4, 1, 4, 1, 2, 4, 2, 3, 4, 1, 2, 3, 4, 1, 3, 4, 1, 2, 3, 4, 4, 2, 4, 2, 1, 3, 4, 3, 4, 3, 1, 3, 4, 3, 1, 2, 4, 1, 2, 1, 3, 4, 3, 1, 2, 3, 4, 1, 2, 3, 1, 2, 1, 2, 3, 4, 1, 2, 3, 4, 1, 1, 2, 4, 1, 1, 1, 2, 1, 3, 1, 2, 3, 4, 4, 3, 3, 4, 2, 4, 1, 3, 1, 2, 3, 4, 1, 3, 1, 2, 3, 4, 2, 3, 3, 2, 3, 2, 2, 3, 2, 1, 2, 1, 2, 3, 1, 1, 2, 3, 4, 2, 3, 4, 1, 2, 3, 4, 1, 3, 4, 3, 3, 1, 1, 1, 1, 1, 3, 2, 4, 1, 2, 4, 1, 2, 3, 4, 2, 4, 1, 1, 2, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 4, 1, 2, 4, 1, 2, 3, 4, 1, 2, 3, 4, 4, 3, 1, 3, 1, 2, 3, 4, 2, 3, 4, 2, 4, 2, 3, 1, 2, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 4, 2, 4, 1, 1, 1, 1, 2, 3, 4, 1, 1, 2, 3, 4, 4, 4, 1, 2, 3, 1, 2, 3, 4, 2, 2, 3, 1, 4, 4, 4, 4, 3, 2, 2, 2, 3, 4, 4, 3, 1, 2, 3, 4, 1, 2, 4, 3, 2, 4, 4, 1, 2, 3, 1, 2, 3, 3, 3, 1, 1, 2, 3, 4, 4, 1, 2, 3, 4, 3, 4, 2, 2, 4, 3, 2, 1, 4, 4, 1, 2, 3, 1, 1, 1, 4, 2, 3, 4, 1, 2, 3, 4, 4, 4, 4, 2, 4, 3, 4, 1, 4, 3, 1, 2, 3, 1, 2, 3, 4, 3, 4, 1, 3, 4, 1, 3, 3, 4, 1, 2, 3, 4, 1, 3, 4, 4, 3, 4, 4, 1, 2, 4, 4, 1, 2, 4, 1, 2, 3, 4, 1, 2, 3, 1, 3, 4, 1, 2, 3, 4, 3, 2, 3, 3, 4, 4, 1, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 3, 1, 2, 3, 4, 4, 1, 2, 3, 4, 1, 4, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 3, 4, 4, 4, 1, 2, 3, 4, 3, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 4], \"Freq\": [0.9946123080218531, 0.8958310159589277, 0.10451361852854156, 0.23732892712841144, 0.6856169005931887, 0.07383566621772801, 0.9949787892632189, 0.9931452671051187, 0.992705081416608, 0.9956467376315901, 0.9860872386584749, 0.9977126988814685, 0.9960215176740482, 0.9841145353820929, 0.7885499739619003, 0.20901324611038322, 0.9980963648926442, 0.8702486707521584, 0.08219015223770385, 0.04834714837511991, 0.9972421334183732, 0.09336161444403757, 0.8335858432503354, 0.07335555420602952, 0.08094892329291682, 0.27452243551510924, 0.6475913863433346, 0.04313143696551085, 0.8841944577929725, 0.06469715544826628, 0.8787885519593364, 0.10815859101037986, 0.12670340772685984, 0.1079325325080658, 0.7649131651658576, 0.9875876642891855, 0.9968148932450759, 0.9961601695362687, 0.9197889846248063, 0.0707529988172928, 0.7109995881341875, 0.25894429444393247, 0.00877777269301466, 0.02194443173253665, 0.1724049331165808, 0.8254539221945385, 0.9759062408777247, 0.020247017445595948, 0.9949792291688201, 0.010390335679571707, 0.046756510558072686, 0.8104461830065932, 0.1350743638344322, 0.9806867827351132, 0.00899712644711113, 0.9501483208310918, 0.01919491557234529, 0.02879237335851793, 0.21110701088153333, 0.1005271480388254, 0.6886109640659539, 0.19362562261497954, 0.6652286242316624, 0.095854268621277, 0.04601004893821296, 0.0067643233564778785, 0.5659483874919825, 0.42840714591026563, 0.14280338575953133, 0.6087933813958967, 0.05261177370087996, 0.19917314329618843, 0.9929220809868992, 0.9417220707253909, 0.03622007964328427, 0.9763741840191339, 0.05074483289446673, 0.21425596110997064, 0.7329809195867417, 0.0250527834112149, 0.9687076252336428, 0.992028610956269, 0.005392133640407515, 0.9759761889137601, 0.016176400921222543, 0.9962749283026139, 0.08214215737168953, 0.09285635181147511, 0.8249929718634905, 0.04829788346964378, 0.9176597859232318, 0.032156764390896576, 0.9578121965002767, 0.009187646968827594, 0.9943218283550617, 0.10262467275374212, 0.8402395081712637, 0.012828084094217766, 0.0384842522826533, 0.45837745036565136, 0.4649570309929095, 0.07676177398467846, 0.9065036014073842, 0.08657618665126703, 0.6394075572696022, 0.14668761607949699, 0.07898563942742144, 0.13916517422926636, 0.8026007969252112, 0.04504392227641492, 0.0286643141759004, 0.12284706075385886, 0.9970224317701915, 0.04605784058783526, 0.8566758349337358, 0.09211568117567052, 0.9933448092104079, 0.9935504113370458, 0.9933486083901568, 0.989877557903706, 0.003660344286193037, 0.9956136458445061, 0.1919232007178855, 0.36198173299955616, 0.07774104332876373, 0.36684054820760387, 0.9946244781327466, 0.99777460455221, 0.9967008326208343, 0.9834540103899271, 0.03378925651825061, 0.9573622680171006, 0.9941259921295545, 0.9982915836355059, 0.2926489936210215, 0.5501801080075204, 0.10535363770356772, 0.05150622287729978, 0.9975598892126127, 0.0012723978178732305, 0.2731388071010832, 0.4913167566757289, 0.17154449470372907, 0.0632882601819583, 0.013674747740651163, 0.9845818373268838, 0.9945169473423149, 0.9684712807107034, 0.015372560011281006, 0.9930546977233948, 0.9637981694842099, 0.03381747963102491, 0.9939429396829341, 0.1938664271969819, 0.8019020397693343, 0.14570324062837758, 0.7814991997340253, 0.07285162031418879, 0.9917492633013287, 0.7391289552930309, 0.09303721115576613, 0.07753100929647178, 0.09303721115576613, 0.0035455982220120676, 0.9466747252772221, 0.049638375108168944, 0.14681811499738728, 0.10749183419451569, 0.3985063121357655, 0.34607127106527, 0.0883784790305843, 0.5710609414283908, 0.3399172270407088, 0.990728366158406, 0.9935926821984319, 0.9961594962655597, 0.9912211941032114, 0.9922419406244024, 0.9979550335699737, 0.9931406648636103, 0.9970962913139162, 0.9814108953471465, 0.9823691815870911, 0.1031166652640929, 0.8936777656221384, 0.9852974627645625, 0.2642755453545252, 0.5408429765394933, 0.18560747603968977, 0.009833508664354425, 0.988344983033672, 0.9968976825299709, 0.9983652389271729, 0.6194655627571712, 0.3182889355603145, 0.06160431010844797, 0.16486412912922188, 0.6638334402990792, 0.1692410529114136, 0.0014589745940639103, 0.05837281189345514, 0.37577497656411746, 0.5363002092711191, 0.02918640594672757, 0.13370392830268396, 0.8022235698161039, 0.05656704658959707, 0.08326574473648206, 0.7202486919705698, 0.19567450013073284, 0.11629827465131713, 0.49367430872395845, 0.32753391636493395, 0.06170928859049481, 0.0678476228249895, 0.6941333719787387, 0.19310477265573936, 0.046971431186531194, 0.9964509148112054, 0.9908880526426361, 0.033062412742442926, 0.9629427711236502, 0.5379115717964995, 0.19046674332093952, 0.2051180312687041, 0.06488427519724313, 0.01790055960485271, 0.04475139901213177, 0.9308290994523408, 0.8719618255294115, 0.09426614330047692, 0.9241590771565846, 0.04508093059300412, 0.09657896238104588, 0.9014036488897615, 0.42580542276601047, 0.16047943069070744, 0.2075533970266483, 0.20541367128410554, 0.06650140757241747, 0.906081678174188, 0.016625351893104368, 0.5613072848318394, 0.32467774318704434, 0.06878765745488229, 0.04677560706931995, 0.31142234720660894, 0.5459050556915851, 0.07327584640155504, 0.07327584640155504, 0.016493986432713888, 0.9813921927464764, 0.09268064036640866, 0.9049991941661081, 0.9961571444237274, 0.9914874443954802, 0.9931395677642155, 0.20175763972617522, 0.5479837128365252, 0.17435845408434897, 0.07721588680878311, 0.9962918505789656, 0.6244398289058891, 0.19899730811286576, 0.08920568984369845, 0.08577470177278697, 0.9970668180553043, 0.9942344654066956, 0.27328270417955947, 0.7126784246251257, 0.01607545318703291, 0.09816556091115426, 0.34490602482297444, 0.19633112182230852, 0.3608247644301886, 0.980275312006545, 0.8933807935086592, 0.09658170740634153, 0.873765165516925, 0.1184766326124644, 0.9979718193966969, 0.989127050962263, 0.9868054007307487, 0.991331577313783, 0.9939457330692482, 0.9770854488770757, 0.9951482476588106, 0.9648741020087044, 0.032523846135124865, 0.9882526310374706, 0.9908871716633013, 0.5522370682034747, 0.3986262975077955, 0.043163357054984225, 0.007617063009703099, 0.8191634831947406, 0.08250567456637675, 0.10018546197345748, 0.9959738660315433, 0.8459055028196131, 0.1446943623244075, 0.9907063870536801, 0.23008440077230913, 0.21291392310273383, 0.55632347649424, 0.28969531949324595, 0.5773505311027366, 0.13260701244409145, 0.9946114231558871, 0.991037638902004, 0.9912251505457895, 0.09953487387439758, 0.7435840577675583, 0.13466482935947907, 0.023419970323387666, 0.993862254484187, 0.11531918851721037, 0.6726952663503938, 0.15375891802294717, 0.06086290505074992, 0.6626865647377554, 0.33603652999449923, 0.9814095005946117, 0.9808522871040585, 0.9863340744069612, 0.9982913672970231, 0.9843846054543395, 0.9765074010596101, 0.010388376607017129, 0.98529977399821, 0.29158679470695864, 0.7013844521329546, 0.007880724181269153, 0.997794451103243, 0.9950646444480492, 0.09253472121148239, 0.9022135318119533, 0.12492263706559091, 0.16656351608745457, 0.7078949433716819, 0.31325278421417474, 0.6230632301402816, 0.044750397744882105, 0.01721169144033927, 0.995468194246591, 0.9983980693192003, 0.9827433116774155, 0.970357184165311, 0.01470238157826229, 0.058961387848587093, 0.933555307602629, 0.38654201715278436, 0.6134253750468099, 0.9869698032694598, 0.3885720697089441, 0.5304317142058602, 0.08018153819390911, 0.02273108259386265, 0.09471284414109438, 0.6932980191128109, 0.19321420204783252, 0.9815658162712048, 0.017846651204930997, 0.01699423687413149, 0.09346830280772318, 0.8837003174548373, 0.9917497383099799, 0.9953288563274354, 0.989396546295513, 0.996962870457867, 0.27278996870092825, 0.5492171369845356, 0.0909299895669761, 0.08729278998429706, 0.0034061435687941614, 0.9605324863999536, 0.034061435687941614, 0.9929219742871934, 0.020117223827749022, 0.9790382262837858, 0.9933084772267906, 0.9709958413378629, 0.014712058202088832, 0.014712058202088832, 0.9942927476527064, 0.05632721769913232, 0.9012354831861171, 0.037551478466088216, 0.22345090455706204, 0.6256625327597738, 0.05687841206907034, 0.09750584926126345, 0.12113328324323, 0.748823932776331, 0.12663934157246773, 0.2253943270474773, 0.36567471241269045, 0.40823168325382253, 0.35291117934346644, 0.5252148727876295, 0.04151896227570194, 0.08096197643761878, 0.9907319471653602, 0.9468451264241764, 0.02254393158152801, 0.08065602932086888, 0.9160220472870108, 0.9876695959553098, 0.04313353675385539, 0.9273710402078909, 0.02695846047115962, 0.3529447762298577, 0.5601972320392509, 0.07182015795375012, 0.016416036103714313, 0.35246510650488716, 0.6304470206917604, 0.011970513051109376, 0.003990171017036459, 0.9940191091860756, 0.19005905790503289, 0.4812785821143574, 0.13028241872522414, 0.19772272959475196, 0.982743376943336, 0.7389565145466875, 0.1776337775352614, 0.0639481599126941, 0.014210702202820913, 0.9824888456057947, 0.00839734056073329, 0.9808547891472268, 0.9860152986708424, 0.39473637849405374, 0.4692670233845394, 0.008281182765609518, 0.12697813573934597, 0.18382920915306517, 0.1994546919310757, 0.4228071810520499, 0.19393981565648377, 0.011712922193264847, 0.8140480924319069, 0.1698373718023403, 0.9926732708376682, 0.9858331576300298, 0.10852417137856446, 0.10852417137856446, 0.763688613404713, 0.02411648252856988, 0.9944184003031274, 0.9893962083734612, 0.21142341801284886, 0.5379254053238306, 0.24086212178678984, 0.010704983190523994, 0.4084770275968819, 0.4684793028371019, 0.0761567339587407, 0.04846337615556226, 0.3349511019431941, 0.46007651358864016, 0.13090043064446666, 0.07507524698726764, 0.037703250924723306, 0.9425812731180827, 0.018851625462361653, 0.5807247429915408, 0.279473782564679, 0.01814764821848565, 0.11977447824200528, 0.271869938278123, 0.2313786708749983, 0.44540394143437173, 0.05206020094687462, 0.01816908975953403, 0.4951076959473023, 0.4860231510675353], \"Term\": [\"3d\", \"accept\", \"accept\", \"actually\", \"actually\", \"actually\", \"amiga\", \"animation\", \"argue\", \"argument\", \"astronaut\", \"atheism\", \"atheist\", \"atmosphere\", \"available\", \"available\", \"belief\", \"believe\", \"believe\", \"believe\", \"bible\", \"big\", \"big\", \"big\", \"bit\", \"bit\", \"bit\", \"burn\", \"burn\", \"burn\", \"buy\", \"buy\", \"center\", \"center\", \"center\", \"christ\", \"christian\", \"christianity\", \"church\", \"church\", \"claim\", \"claim\", \"claim\", \"claim\", \"code\", \"code\", \"color\", \"color\", \"comp\", \"computer\", \"computer\", \"computer\", \"computer\", \"conclusion\", \"conclusion\", \"context\", \"context\", \"context\", \"cost\", \"cost\", \"cost\", \"could\", \"could\", \"could\", \"could\", \"data\", \"data\", \"data\", \"day\", \"day\", \"day\", \"day\", \"dc\", \"dead\", \"dead\", \"den\", \"design\", \"design\", \"design\", \"development\", \"development\", \"directory\", \"display\", \"display\", \"display\", \"driver\", \"earth\", \"earth\", \"earth\", \"eat\", \"eat\", \"edu\", \"edu\", \"edu\", \"email\", \"enough\", \"enough\", \"enough\", \"enough\", \"even\", \"even\", \"even\", \"evidence\", \"evidence\", \"example\", \"example\", \"example\", \"example\", \"exist\", \"exist\", \"exist\", \"exist\", \"existence\", \"face\", \"face\", \"face\", \"faith\", \"fallacy\", \"false\", \"fbi\", \"file\", \"file\", \"first\", \"first\", \"first\", \"first\", \"flight\", \"format\", \"ftp\", \"fuel\", \"fund\", \"fund\", \"gay\", \"gif\", \"give\", \"give\", \"give\", \"give\", \"god\", \"god\", \"good\", \"good\", \"good\", \"good\", \"graphic\", \"graphic\", \"graphics\", \"greek\", \"greek\", \"gun\", \"guy\", \"guy\", \"hang\", \"happen\", \"happen\", \"hear\", \"hear\", \"hear\", \"homosexual\", \"human\", \"human\", \"human\", \"human\", \"image\", \"image\", \"image\", \"include\", \"include\", \"include\", \"include\", \"information\", \"information\", \"information\", \"input\", \"internet\", \"interpretation\", \"islam\", \"islamic\", \"jesus\", \"jew\", \"jpeg\", \"juda\", \"jupiter\", \"king\", \"king\", \"km\", \"know\", \"know\", \"know\", \"know\", \"koresh\", \"launch\", \"law\", \"life\", \"life\", \"life\", \"like\", \"like\", \"like\", \"like\", \"line\", \"line\", \"line\", \"line\", \"little\", \"little\", \"little\", \"long\", \"long\", \"long\", \"look\", \"look\", \"look\", \"look\", \"lot\", \"lot\", \"lot\", \"lot\", \"lunar\", \"mac\", \"mail\", \"mail\", \"many\", \"many\", \"many\", \"many\", \"mar\", \"mar\", \"mar\", \"mary\", \"mary\", \"master\", \"master\", \"matthew\", \"matthew\", \"may\", \"may\", \"may\", \"may\", \"maybe\", \"maybe\", \"maybe\", \"mean\", \"mean\", \"mean\", \"mean\", \"might\", \"might\", \"might\", \"might\", \"mission\", \"mission\", \"moon\", \"moon\", \"moral\", \"morality\", \"mormon\", \"much\", \"much\", \"much\", \"much\", \"muslim\", \"must\", \"must\", \"must\", \"must\", \"nasa\", \"national\", \"never\", \"never\", \"never\", \"new\", \"new\", \"new\", \"new\", \"night\", \"normal\", \"normal\", \"objective\", \"objective\", \"orbit\", \"orbital\", \"orbiter\", \"output\", \"p1\", \"p2\", \"p3\", \"package\", \"package\", \"payload\", \"pc\", \"people\", \"people\", \"people\", \"people\", \"person\", \"person\", \"person\", \"pixel\", \"plane\", \"plane\", \"planetary\", \"please\", \"please\", \"please\", \"point\", \"point\", \"point\", \"polygon\", \"postscript\", \"premise\", \"probably\", \"probably\", \"probably\", \"probably\", \"probe\", \"problem\", \"problem\", \"problem\", \"problem\", \"program\", \"program\", \"prophecy\", \"prophet\", \"propulsion\", \"pub\", \"quran\", \"ra\", \"ra\", \"radio\", \"really\", \"really\", \"really\", \"religion\", \"religious\", \"report\", \"report\", \"research\", \"research\", \"research\", \"right\", \"right\", \"right\", \"right\", \"rocket\", \"satellite\", \"saturn\", \"saw\", \"saw\", \"sci\", \"sci\", \"science\", \"science\", \"screen\", \"seem\", \"seem\", \"seem\", \"send\", \"send\", \"send\", \"send\", \"server\", \"server\", \"service\", \"service\", \"service\", \"sex\", \"sgi\", \"shareware\", \"shuttle\", \"since\", \"since\", \"since\", \"since\", \"software\", \"software\", \"software\", \"solar\", \"space\", \"space\", \"spacecraft\", \"statement\", \"statement\", \"statement\", \"station\", \"stay\", \"stay\", \"stay\", \"still\", \"still\", \"still\", \"still\", \"sure\", \"sure\", \"sure\", \"system\", \"system\", \"system\", \"take\", \"take\", \"take\", \"take\", \"tar\", \"tax\", \"tax\", \"technology\", \"technology\", \"telescope\", \"thanks\", \"thanks\", \"thanks\", \"thing\", \"thing\", \"thing\", \"thing\", \"think\", \"think\", \"think\", \"think\", \"tiff\", \"time\", \"time\", \"time\", \"time\", \"titan\", \"true\", \"true\", \"true\", \"true\", \"truth\", \"truth\", \"tyre\", \"unix\", \"us\", \"us\", \"us\", \"us\", \"use\", \"use\", \"use\", \"use\", \"user\", \"user\", \"user\", \"vehicle\", \"venus\", \"version\", \"version\", \"version\", \"version\", \"viewer\", \"visualization\", \"want\", \"want\", \"want\", \"want\", \"way\", \"way\", \"way\", \"way\", \"well\", \"well\", \"well\", \"well\", \"window\", \"window\", \"window\", \"word\", \"word\", \"word\", \"word\", \"write\", \"write\", \"write\", \"write\", \"year\", \"year\", \"year\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 1, 4, 3]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el3738818898202018082505376210\", ldavis_el3738818898202018082505376210_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el3738818898202018082505376210\", ldavis_el3738818898202018082505376210_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el3738818898202018082505376210\", ldavis_el3738818898202018082505376210_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=                x           y  topics  cluster       Freq\n",
       "topic                                                    \n",
       "1      269.562408 -124.803398       1        1  28.194162\n",
       "0        1.782994 -295.079681       2        1  26.968686\n",
       "3      220.811432 -343.832123       3        1  23.756904\n",
       "2       50.533993  -76.050934       4        1  21.080248, topic_info=        Term         Freq        Total Category  logprob  loglift\n",
       "846    space  1043.000000  1043.000000  Default  30.0000  30.0000\n",
       "416    image   846.000000   846.000000  Default  29.0000  29.0000\n",
       "368      god   785.000000   785.000000  Default  28.0000  28.0000\n",
       "329     file   546.000000   546.000000  Default  27.0000  27.0000\n",
       "576     nasa   413.000000   413.000000  Default  26.0000  26.0000\n",
       "..       ...          ...          ...      ...      ...      ...\n",
       "943      use   210.876748  1087.966384   Topic4  -4.7590  -0.0840\n",
       "334    first   151.140257   411.622981   Topic4  -5.0921   0.5549\n",
       "589      new   136.220459   376.914263   Topic4  -5.1960   0.5391\n",
       "421  include   131.866526   381.424322   Topic4  -5.2285   0.4947\n",
       "913     time   128.829956   652.428784   Topic4  -5.2518  -0.0654\n",
       "\n",
       "[275 rows x 6 columns], token_table=      Topic      Freq      Term\n",
       "term                           \n",
       "0         3  0.994612        3d\n",
       "3         1  0.895831    accept\n",
       "3         3  0.104514    accept\n",
       "11        1  0.237329  actually\n",
       "11        2  0.685617  actually\n",
       "...     ...       ...       ...\n",
       "993       3  0.445404     write\n",
       "993       4  0.052060     write\n",
       "996       1  0.018169      year\n",
       "996       2  0.495108      year\n",
       "996       4  0.486023      year\n",
       "\n",
       "[478 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[2, 1, 4, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare to display result in the Jupyter notebook\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "#run the visualization [mds is a function to use for visualizing the \"distance\" between topics]\n",
    "pyLDAvis.sklearn.prepare(lda_news, bow_news_corpus, bow_vectorizer_news, mds='tsne')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font color=green>EXERCISE 2:</font>**\n",
    "    \n",
    "**<font color=green>2.1. Fit a topic model with 3 topics (n_components = 3). The script is provided below. Important! Note that the model with three topics is called lda_news_3_topics.</font>**\n",
    "\n",
    "**<font color=green>2.2. Use the visualization tool to answer the following question: Which topic is the most common / largest topic in the corpus? Can you give a name to that topic? List 5 most relevant and exclusive terms for that topic (with lambda = 0.2).</font>**\n",
    "\n",
    "**<font color=green>2.3. You fit the model with 3 topics, but you know that the dataset has four classes (topics). Which 2 of the 4 classes ('atheism', 'religion','computer graphics', 'space science') were grouped together into one topic by the topic model when you fit it with 3 topics? Why?</font>**\n",
    "\n",
    "Your answer (need to add lines of code related to visualization):\n",
    "\n",
    "**Answer 2.1**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the LDA model with 3 topics\n",
    "lda_news_3_topics = LatentDirichletAllocation(n_components=3, max_iter=100,\n",
    "                                     doc_topic_prior = 0.25,\n",
    "                                     topic_word_prior = 0.25).fit(bow_news_corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion: fit topic model with 3 topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 2.2:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el3738818898202022565947463322\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el3738818898202022565947463322_data = {\"mdsDat\": {\"x\": [-1015.7005615234375, -387.9960021972656, -2184.74560546875], \"y\": [1338.981201171875, -373.316162109375, -60.565799713134766], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [48.29631091957015, 27.138814200892092, 24.564874879537758]}, \"tinfo\": {\"Term\": [\"space\", \"image\", \"file\", \"launch\", \"god\", \"nasa\", \"edu\", \"satellite\", \"graphic\", \"software\", \"program\", \"format\", \"orbit\", \"jpeg\", \"ftp\", \"mission\", \"color\", \"year\", \"earth\", \"data\", \"jesus\", \"mail\", \"people\", \"shuttle\", \"christian\", \"available\", \"moon\", \"gif\", \"pub\", \"lunar\", \"jesus\", \"christian\", \"religion\", \"bible\", \"atheist\", \"god\", \"argument\", \"belief\", \"atheism\", \"moral\", \"child\", \"religious\", \"faith\", \"morality\", \"islam\", \"church\", \"christ\", \"existence\", \"muslim\", \"christianity\", \"interpretation\", \"woman\", \"indeed\", \"koresh\", \"verse\", \"gay\", \"fallacy\", \"evil\", \"false\", \"mormon\", \"law\", \"evidence\", \"statement\", \"love\", \"truth\", \"believe\", \"agree\", \"conclusion\", \"people\", \"claim\", \"think\", \"matthew\", \"kill\", \"true\", \"life\", \"something\", \"nothing\", \"even\", \"reason\", \"seem\", \"thing\", \"mean\", \"really\", \"word\", \"know\", \"right\", \"way\", \"take\", \"us\", \"well\", \"many\", \"good\", \"like\", \"point\", \"could\", \"give\", \"time\", \"use\", \"may\", \"file\", \"graphic\", \"format\", \"jpeg\", \"ftp\", \"color\", \"gif\", \"pub\", \"3d\", \"graphics\", \"info\", \"directory\", \"mac\", \"pc\", \"server\", \"screen\", \"unix\", \"software\", \"hi\", \"pixel\", \"anonymous\", \"sgi\", \"comp\", \"amiga\", \"polygon\", \"viewer\", \"tiff\", \"op\", \"animation\", \"internet\", \"disk\", \"image\", \"edu\", \"display\", \"package\", \"mail\", \"window\", \"email\", \"thanks\", \"algorithm\", \"available\", \"version\", \"program\", \"computer\", \"code\", \"user\", \"send\", \"use\", \"data\", \"line\", \"bit\", \"information\", \"system\", \"please\", \"know\", \"look\", \"work\", \"write\", \"launch\", \"satellite\", \"orbit\", \"shuttle\", \"lunar\", \"moon\", \"rocket\", \"flight\", \"station\", \"probe\", \"spacecraft\", \"dc\", \"solar\", \"vehicle\", \"fund\", \"planetary\", \"orbital\", \"payload\", \"orbiter\", \"russian\", \"propulsion\", \"astronaut\", \"venus\", \"km\", \"energy\", \"radio\", \"team\", \"atmosphere\", \"fuel\", \"sky\", \"mission\", \"nasa\", \"space\", \"star\", \"mar\", \"earth\", \"technology\", \"national\", \"development\", \"year\", \"cost\", \"report\", \"service\", \"center\", \"system\", \"build\", \"first\", \"research\", \"use\", \"new\", \"data\", \"high\", \"time\", \"science\", \"program\", \"design\", \"include\"], \"Freq\": [1043.0, 847.0, 547.0, 399.0, 784.0, 413.0, 436.0, 284.0, 292.0, 293.0, 533.0, 282.0, 253.0, 265.0, 256.0, 242.0, 247.0, 442.0, 280.0, 443.0, 408.0, 242.0, 788.0, 201.0, 365.0, 316.0, 183.0, 197.0, 197.0, 182.0, 407.52082365626904, 364.9055215084857, 300.49843847560965, 284.6441929837331, 254.91537370522968, 783.0364038516803, 246.98122913579306, 206.3595551891686, 198.41301534811976, 171.67709041215028, 160.76805037602432, 156.81354626926276, 137.98583379630358, 122.13172066147574, 120.15007704025471, 112.21921346798405, 98.34952753320499, 92.40068136961976, 86.45855148879181, 85.46653203035834, 85.45637538406244, 79.50557630187163, 81.46932784142504, 75.55675719337995, 72.58370795097204, 72.58311068669946, 69.61269900167987, 69.61071052601457, 68.62132330661218, 67.6302636799771, 210.57664095217905, 193.16080195896188, 133.7537768500831, 131.71133662267636, 117.09821302803705, 395.0579316621048, 149.97367513816758, 109.06731539900126, 732.119826980627, 217.82092979877356, 680.1375714429472, 122.06565880312117, 140.6091743543036, 257.8741884836143, 266.3352461195603, 269.8264617175134, 155.71190086714847, 386.439180032905, 186.2861859238191, 283.73145254332786, 394.35060799433444, 307.0969364184091, 223.18222338015477, 236.24229447885668, 552.8042839255938, 244.22743995152382, 330.4546611852257, 356.1867722259524, 287.3626551186504, 359.8818176251562, 339.89749311999947, 395.50945951409136, 427.4337630730973, 331.17233856579617, 340.3784198652816, 301.77293334125153, 351.8641206477651, 341.0100697465887, 263.8915501589039, 546.9600006424756, 292.48473395544426, 281.56149951820225, 264.6616103549037, 255.71129388004954, 246.75391940703716, 197.06377318594298, 197.0600760012082, 144.38372589680105, 143.3904074168428, 132.42031091458983, 121.51848660944238, 113.55907793061473, 113.55460750914445, 111.53812430312665, 92.68609745707947, 88.7122513472782, 292.27462724671216, 83.7416461055802, 79.75570355860354, 80.73709736444182, 75.78730700488842, 73.80549475973142, 73.80334497915847, 71.81791678748957, 70.82333612583166, 68.83499319449525, 70.80141616065156, 64.85975044832821, 66.83012768385404, 67.81043499607323, 819.9716961972495, 420.5101077635876, 181.72058694781668, 179.6733539974098, 233.19457916442263, 104.08544804722449, 136.91889234373605, 176.1781653825434, 82.62620853583972, 257.00061094495516, 198.5282252953942, 361.4132423339209, 159.10107465960317, 156.57284678143358, 143.5023515816919, 194.95737408265666, 500.5106112704894, 267.403375044088, 197.2451446323608, 194.42325641438987, 185.74005352761608, 230.9908569067713, 175.35131905647444, 207.37075406480474, 176.75943546083576, 172.41122497605997, 163.15549499913064, 398.61934882371855, 283.796836682187, 252.84994831079422, 200.9222967416031, 181.9619983571368, 182.9487174361876, 154.00633490042276, 136.0306938589968, 130.0271121809559, 123.04944507611184, 115.06701385483525, 110.07378682211376, 110.07319597219714, 107.0791982531706, 88.10599308485213, 88.08287478837853, 77.12340587462376, 72.13012733714828, 65.14570907278683, 66.11874492808951, 63.14270214492697, 62.14930400515811, 61.146795402759686, 59.154207408027204, 60.13315800825833, 59.135448455457755, 60.124431464374375, 55.15358645120094, 53.163975652574074, 52.1585353754537, 239.06831356652202, 404.61766279964513, 1007.807041665774, 100.81704629943107, 105.2115082414731, 242.44116837137324, 154.9690570072457, 117.62159547501516, 108.55474242869262, 302.7711710811414, 156.35144318453814, 111.23451952234916, 102.91353283477216, 155.84878448141492, 250.62725069774064, 115.5363785494189, 179.2205073343891, 127.90511067183473, 248.72388897412358, 165.8731527375759, 176.19206595636516, 140.09637167671733, 194.2638088411565, 136.18494622418496, 171.64864212314598, 122.55191893319862, 130.01269189014653], \"Total\": [1043.0, 847.0, 547.0, 399.0, 784.0, 413.0, 436.0, 284.0, 292.0, 293.0, 533.0, 282.0, 253.0, 265.0, 256.0, 242.0, 247.0, 442.0, 280.0, 443.0, 408.0, 242.0, 788.0, 201.0, 365.0, 316.0, 183.0, 197.0, 197.0, 182.0, 408.01929424192735, 365.40921410800127, 300.998547460214, 285.1436149001714, 255.41562969625653, 784.5770936426367, 247.4881942643252, 206.85992098713524, 198.932511545978, 172.17726566068566, 161.2770614207176, 157.3132688643773, 138.48554032141368, 122.63061068004588, 120.648743020663, 112.72129400719952, 98.84821911383354, 92.90264007020706, 86.95702300419549, 85.96609588187704, 85.9661557258633, 80.02061360199646, 82.00248361334413, 76.05676952012898, 73.08397471354363, 73.08397612191416, 70.1111627588887, 70.11117647604163, 69.12023263798787, 68.12929945504996, 212.81859114069172, 195.97706303132756, 135.5222686526814, 133.54084223083194, 118.67498000891621, 413.1057227670152, 153.37128771354818, 110.74829307454632, 788.7735490598773, 227.73731986674477, 754.2460429158419, 124.62796288068745, 145.45471637344517, 281.26976670342333, 292.2711528672323, 299.107005181391, 163.2948077596688, 456.875272456805, 200.02274054175356, 324.9086307921545, 488.7616758717263, 363.7194602532844, 254.61918127429612, 275.4974202615397, 816.3626397746949, 291.3273851028501, 434.3258582840556, 483.0600661839226, 362.96846923369674, 520.7943011517214, 477.9710700355513, 602.1949900622967, 688.7192801788174, 491.86445935805875, 523.976713944317, 428.4555269666949, 654.5772200151952, 1090.2445699912016, 467.528544235981, 547.4733457184162, 292.9965400227113, 282.06202330355245, 265.163176570616, 256.21674517459934, 247.27028476802505, 197.56778568774732, 197.56779962747467, 144.88311036860594, 143.88905971250836, 132.95453846414358, 122.01997288040482, 114.06757703344, 114.06762056300053, 112.07959714439556, 93.19250940859814, 89.21633473207491, 293.98691423087877, 84.2460525936523, 80.26992725573056, 81.26398517115767, 76.29366979798617, 74.30555727429736, 74.30557353637043, 72.31745264459045, 71.32341521267622, 69.33530004295784, 71.3235151563796, 65.35911000409953, 67.34728497646468, 68.34135349107466, 847.7707341074318, 436.09243267709337, 185.65410053329182, 184.6648509997986, 242.27311061012892, 106.12180795161956, 141.8870349041378, 185.61121113895783, 84.24258685950421, 316.11720377780534, 249.13909655980507, 533.3157240201035, 192.71582010100968, 191.49521678545284, 170.8380186197688, 264.3122963188235, 1090.2445699912016, 443.8711169515359, 275.01266083790887, 284.8482857514458, 294.30695731805775, 634.5755122109708, 291.638459609826, 816.3626397746949, 422.9333686443616, 418.3851184070687, 346.2604030781673, 399.1222390404603, 284.30196337896695, 253.35042895484088, 201.43165969541928, 182.46139881509416, 183.4597524183761, 154.50515932096323, 136.5332738629984, 130.54258287026474, 123.55358744879707, 115.5661100286995, 110.57391833404078, 110.57391764940509, 107.57860758587782, 88.60827905363865, 88.60818155466148, 77.62548383929474, 72.63329590703812, 65.6442573494475, 66.64250419342099, 63.64735679723138, 62.64893616144825, 61.6504841599935, 59.65362815727243, 60.6519336050753, 59.6535280725003, 60.65186483869369, 55.65982747056944, 53.66300402930389, 52.66452013970404, 242.34660484971934, 413.0652954802911, 1043.9553034681924, 103.56783590196898, 111.54339138824999, 280.0274151324192, 173.39633436038986, 129.49401058896314, 119.51368636916342, 442.0300762762794, 199.20330306161793, 129.42998766903546, 117.50068590102973, 213.16216133446744, 634.5755122109708, 152.2883229398613, 412.50360572421715, 192.1048022377717, 1090.2445699912016, 377.8250352139909, 443.8711169515359, 246.72473896176564, 654.5772200151952, 237.63355418433113, 533.3157240201035, 177.1995224397677, 381.7578609128785], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.9292, -5.0397, -5.2339, -5.2881, -5.3984, -4.2762, -5.43, -5.6097, -5.649, -5.7937, -5.8594, -5.8843, -6.0122, -6.1342, -6.1506, -6.2189, -6.3508, -6.4132, -6.4797, -6.4912, -6.4913, -6.5635, -6.5391, -6.6145, -6.6546, -6.6546, -6.6964, -6.6964, -6.7107, -6.7253, -5.5895, -5.6758, -6.0433, -6.0587, -6.1763, -4.9603, -5.9289, -6.2474, -4.3434, -5.5557, -4.417, -6.1348, -5.9934, -5.3869, -5.3546, -5.3416, -5.8913, -4.9824, -5.7121, -5.2913, -4.9621, -5.2122, -5.5314, -5.4745, -4.6243, -5.4412, -5.1389, -5.0639, -5.2786, -5.0536, -5.1107, -4.9592, -4.8815, -5.1367, -5.1093, -5.2297, -5.0761, -5.1074, -5.3638, -4.0586, -4.6845, -4.7226, -4.7845, -4.8189, -4.8546, -5.0794, -5.0794, -5.3905, -5.3974, -5.477, -5.5629, -5.6306, -5.6307, -5.6486, -5.8337, -5.8776, -4.6853, -5.9352, -5.984, -5.9718, -6.035, -6.0615, -6.0615, -6.0888, -6.1028, -6.1312, -6.1031, -6.1907, -6.1608, -6.1462, -3.6537, -4.3215, -5.1605, -5.1718, -4.9111, -5.7177, -5.4436, -5.1915, -5.9486, -4.8139, -5.072, -4.4729, -5.2934, -5.3094, -5.3966, -5.0902, -4.1473, -4.7742, -5.0785, -5.0929, -5.1386, -4.9206, -5.1962, -5.0284, -5.1882, -5.2131, -5.2682, -4.2753, -4.615, -4.7305, -4.9604, -5.0595, -5.0541, -5.2263, -5.3504, -5.3956, -5.4507, -5.5178, -5.5622, -5.5622, -5.5897, -5.7848, -5.785, -5.9179, -5.9848, -6.0867, -6.0719, -6.1179, -6.1338, -6.15, -6.1832, -6.1667, -6.1835, -6.1669, -6.2532, -6.2899, -6.309, -4.7866, -4.2604, -3.3478, -5.65, -5.6073, -4.7725, -5.2201, -5.4958, -5.576, -4.5503, -5.2112, -5.5517, -5.6294, -5.2144, -4.7393, -5.5137, -5.0747, -5.412, -4.747, -5.1521, -5.0917, -5.321, -4.9941, -5.3493, -5.1179, -5.4548, -5.3957], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.7266, 0.7264, 0.7262, 0.7261, 0.7259, 0.7258, 0.7258, 0.7254, 0.7252, 0.7249, 0.7247, 0.7246, 0.7242, 0.7237, 0.7237, 0.7234, 0.7228, 0.7224, 0.7221, 0.722, 0.7219, 0.7214, 0.7213, 0.7212, 0.7209, 0.7209, 0.7207, 0.7207, 0.7206, 0.7205, 0.7172, 0.7133, 0.7147, 0.714, 0.7144, 0.6831, 0.7054, 0.7125, 0.6533, 0.6833, 0.6244, 0.707, 0.6939, 0.641, 0.6349, 0.6248, 0.6803, 0.5604, 0.6567, 0.5923, 0.5132, 0.5586, 0.596, 0.5741, 0.338, 0.5515, 0.4545, 0.4231, 0.4942, 0.3582, 0.3869, 0.3074, 0.2508, 0.3323, 0.2964, 0.3773, 0.1071, -0.4344, 0.1559, 1.3033, 1.3025, 1.3024, 1.3023, 1.3022, 1.3021, 1.3017, 1.3016, 1.3008, 1.3007, 1.3002, 1.3001, 1.2997, 1.2997, 1.2994, 1.2988, 1.2985, 1.2984, 1.2982, 1.2978, 1.2977, 1.2975, 1.2975, 1.2974, 1.2973, 1.2972, 1.297, 1.2969, 1.2965, 1.2965, 1.2964, 1.2709, 1.2678, 1.2828, 1.2768, 1.266, 1.2848, 1.2686, 1.252, 1.2848, 1.0972, 1.0771, 0.9151, 1.1125, 1.1029, 1.1298, 0.9999, 0.5257, 0.7974, 0.9718, 0.9223, 0.8439, 0.2936, 0.7955, -0.0661, 0.4318, 0.4177, 0.5517, 1.4026, 1.4021, 1.4019, 1.4013, 1.4011, 1.4011, 1.4006, 1.4002, 1.3999, 1.3998, 1.3995, 1.3993, 1.3993, 1.3992, 1.3982, 1.3979, 1.3974, 1.3969, 1.3962, 1.396, 1.3959, 1.3958, 1.3956, 1.3954, 1.3953, 1.3951, 1.3951, 1.3947, 1.3945, 1.3942, 1.3902, 1.3832, 1.3686, 1.3769, 1.3454, 1.2597, 1.2915, 1.3077, 1.3077, 1.0255, 1.1616, 1.2524, 1.2713, 1.0907, 0.4749, 1.1277, 0.5702, 0.9971, -0.074, 0.5806, 0.4799, 0.8379, 0.1891, 0.8471, 0.2702, 1.0351, 0.3267]}, \"token.table\": {\"Topic\": [2, 1, 3, 1, 2, 2, 2, 2, 1, 3, 1, 1, 3, 2, 3, 1, 1, 3, 1, 1, 2, 3, 1, 2, 3, 2, 3, 1, 1, 1, 1, 1, 1, 2, 3, 1, 2, 2, 2, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 2, 3, 3, 1, 2, 3, 2, 3, 2, 2, 2, 3, 1, 3, 1, 2, 1, 2, 3, 1, 2, 3, 1, 3, 1, 1, 1, 1, 1, 2, 1, 2, 3, 3, 2, 2, 3, 3, 1, 2, 1, 2, 3, 1, 2, 1, 2, 3, 2, 2, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 2, 1, 1, 1, 2, 1, 3, 3, 1, 2, 3, 1, 3, 1, 3, 1, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 3, 3, 2, 1, 2, 1, 2, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 1, 3, 3, 1, 1, 1, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 1, 2, 3, 2, 3, 1, 2, 1, 2, 3, 2, 3, 2, 3, 3, 2, 3, 1, 2, 3, 1, 3, 1, 1, 1, 2, 3, 1, 2, 3, 1, 2, 3, 3, 3, 3, 1, 2, 3, 2, 1, 2, 1, 2, 3, 2, 2, 3, 2, 3, 3, 1, 2, 3, 1, 2, 2, 3, 3, 1, 3, 1, 3, 3, 1, 2, 3, 1, 2, 3, 3, 2, 3, 1, 2, 1, 2, 3, 1, 2, 3, 2, 1, 2, 3, 1, 2, 3, 1, 3, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 3, 3, 1, 1, 2, 3, 2, 1, 2, 3, 1, 2, 3, 2, 3, 1, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3], \"Freq\": [0.9939046700035693, 0.9780187819779883, 0.019560375639559768, 0.011870480682979887, 0.9852498966873305, 0.9958876094776273, 0.9945055860754988, 0.9967515108863826, 0.9980274038291953, 0.9896417050119427, 0.9953124225963312, 0.9983727319398942, 0.9881453554465592, 0.812989602997507, 0.186639636485809, 0.9958429792342968, 0.9561716970519275, 0.04357238113148024, 0.999496341868915, 0.27734061938127363, 0.6810643058223681, 0.042127689019940295, 0.1247633412280934, 0.11819684958450954, 0.7617130306557282, 0.2674020550512374, 0.7318372032981233, 0.9982820779453885, 0.9914189742472069, 0.9988801209925694, 0.9887618965131961, 0.993601084750203, 0.9572431963613064, 0.004391023836519754, 0.039519214528677786, 0.18277218923547972, 0.8198638202848662, 0.9989069258027562, 0.9958878274316765, 0.020755950382814695, 0.8250490277168842, 0.15566962787111022, 0.9842138147143313, 0.00902948453866359, 0.07027995914138781, 0.14055991828277561, 0.7831195447183212, 0.6488837976035168, 0.1603124676432218, 0.19084817576574023, 0.601525960584528, 0.39651149461751656, 0.9948096409832652, 0.05643356066830893, 0.2539510230073902, 0.6941327962201999, 0.09203966787554625, 0.9120294362213219, 0.9998363146628102, 0.9950051692915441, 0.9803176955273522, 0.021545443857744002, 0.13212992014550953, 0.8642010993300895, 0.03439637763929469, 0.965391665742871, 0.035239301486412175, 0.9655568607276936, 0.9892512313074756, 0.844869537202835, 0.11381662159209176, 0.04158684250480276, 0.9848091251839423, 0.015307913862962833, 0.9984142831196161, 0.9902840213203314, 0.9964939276672007, 0.9984144784580027, 0.9982605290318165, 0.999135399518318, 0.4363598220771446, 0.12848372538938146, 0.4339356008433827, 0.9960941838724712, 0.999780107570576, 0.9991540553898941, 0.9876450444529374, 0.9931351893961237, 0.9988509639681608, 0.997126122126789, 0.7048572862113536, 0.17738130381477774, 0.11903219071781138, 0.9979898805924673, 0.00127457200586522, 0.6575943117013212, 0.23082224577394864, 0.11292023534265112, 0.9965987993488453, 0.9938212139666164, 0.9970793575950782, 0.21481429151785741, 0.218867391357817, 0.5674339775943403, 0.004718256763382338, 0.9672426364933793, 0.02830954058029403, 0.2331320690743047, 0.4269722163945131, 0.34052998853550126, 0.9877749603527741, 0.9928205650204185, 0.07135407260286165, 0.631993214482489, 0.2956097293547126, 0.9948433707968177, 0.9887612082021643, 0.9946228779146759, 0.9999527124275748, 0.9993846182840078, 0.9693738609203432, 0.03437495960710437, 0.9890429437829131, 0.6773950362949247, 0.25356378392956497, 0.06859696570075187, 0.9992535901736669, 0.9996937303199287, 0.9914547355522645, 0.009397675218504877, 0.910113767268827, 0.08895848853003573, 0.6199913553881268, 0.23376723235945762, 0.1466490091199082, 0.2072631849978555, 0.7163306569224129, 0.07272392456065106, 0.4279633942816184, 0.4185056397118589, 0.15368851175859224, 0.9884616406105293, 0.01497669152440196, 0.9974712524507074, 0.9994075701860469, 0.03714815885813673, 0.9617245571050953, 0.7113401235240261, 0.21340203705720784, 0.075318366020191, 0.008965120995104872, 0.04482560497552436, 0.9413377044860116, 0.9789135373800234, 0.016047762907869235, 0.5646714051040878, 0.2352797521267032, 0.19891833588893998, 0.844057119699379, 0.0824811517621543, 0.07423303658593887, 0.012378964425188951, 0.9861908325400531, 0.9974939875786617, 0.9989704467659801, 0.9948576405470964, 0.9981021461238528, 0.9889942988946469, 0.01936739805433911, 0.9804745265009175, 0.0926683786023905, 0.9112390562568399, 0.3308409669814576, 0.23026531301909448, 0.4393568041513757, 0.9553273747049874, 0.024495573710384293, 0.018371680282788218, 0.9954641164885062, 0.9986168211505048, 0.9919422873989467, 0.9901856251336977, 0.9747388256371339, 0.027076078489920385, 0.9912809146393596, 0.9994071888002329, 0.9280230059342831, 0.05451501264368056, 0.01774907388398902, 0.9966372555082726, 0.9931362821808243, 0.3977527523468365, 0.6000580315577275, 0.6729496179333512, 0.278532017090239, 0.048793930001209754, 0.9956102900063337, 0.9955194546736533, 0.6768973494327198, 0.32251064848317956, 0.9898290073648504, 0.9971260517728836, 0.9890446031673763, 0.8758177560855739, 0.0353469049541263, 0.09033097932721165, 0.9298942685028035, 0.06499261016417444, 0.9966825505683014, 0.9980086303803948, 0.09271421728544949, 0.0540832934165122, 0.8576065098904077, 0.09890434689125233, 0.23424713737401867, 0.666302968530542, 0.8375457045133547, 0.08924667343175093, 0.07208385161795267, 0.9967304695637133, 0.9903589428218933, 0.9989378779682768, 0.39556703312649477, 0.03366527941502083, 0.5723097500553541, 0.9979342823814938, 0.8740918925655627, 0.12618932251826787, 0.07945146817788987, 0.7377636330804058, 0.18160335583517684, 0.9992898159306103, 0.11914824064765149, 0.8765906276220073, 0.9961507973235032, 0.9978570414597586, 0.9873820147237409, 0.003401511943537266, 0.9932414875128817, 0.9948096471427846, 0.9026869826611407, 0.09695526850804843, 0.03448423498630836, 0.965558579616634, 0.9951014183261951, 0.019311014685032896, 0.9752062415941612, 0.988767391013925, 0.014757722253939178, 0.9958436331016679, 0.24110605760206777, 0.3640228712815533, 0.3955400029942419, 0.7369683915549643, 0.10971720436071097, 0.15319005891872853, 0.9892523529090597, 0.1038084228619764, 0.8939058635336857, 0.048488450373087376, 0.9482185850737087, 0.8061188498408453, 0.1084373072120934, 0.08593145099826269, 0.9015625688550994, 0.04905561036417452, 0.04905561036417452, 0.9951640788638674, 0.5377516803774943, 0.16499199284309485, 0.2963745056625963, 0.9172688661986219, 0.0675508079758675, 0.014221222731761579, 0.985885988699637, 0.008426375971791768, 0.997575166781682, 0.7907022904934904, 0.02204048196497534, 0.1873440967022904, 0.3127738577067638, 0.45952991997386705, 0.22838912190317945, 0.005853497998157439, 0.8429037117346713, 0.15219094795209342, 0.9946215367640269, 0.9894488393910195, 0.9988509832165975, 0.1806219923784539, 0.7987505885180517, 0.020069110264272653, 0.9954655114072728, 0.7597981877104243, 0.12433061253443306, 0.11512093753188246, 0.6912518036466039, 0.178573382608706, 0.1305697851332474, 0.9800059196825324, 0.018846267686202546, 0.9997423963517827, 0.8566323407890958, 0.025408586379337586, 0.11978333578830577, 0.2987677967034692, 0.4111044882639736, 0.2892072272089582, 0.46785597937234813, 0.47074397924501693, 0.06064799732604513, 0.29409763492823254, 0.020360605495031485, 0.6854737183327266], \"Term\": [\"3d\", \"agree\", \"agree\", \"algorithm\", \"algorithm\", \"amiga\", \"animation\", \"anonymous\", \"argument\", \"astronaut\", \"atheism\", \"atheist\", \"atmosphere\", \"available\", \"available\", \"belief\", \"believe\", \"believe\", \"bible\", \"bit\", \"bit\", \"bit\", \"build\", \"build\", \"build\", \"center\", \"center\", \"child\", \"christ\", \"christian\", \"christianity\", \"church\", \"claim\", \"claim\", \"claim\", \"code\", \"code\", \"color\", \"comp\", \"computer\", \"computer\", \"computer\", \"conclusion\", \"conclusion\", \"cost\", \"cost\", \"cost\", \"could\", \"could\", \"could\", \"data\", \"data\", \"dc\", \"design\", \"design\", \"design\", \"development\", \"development\", \"directory\", \"disk\", \"display\", \"display\", \"earth\", \"earth\", \"edu\", \"edu\", \"email\", \"email\", \"energy\", \"even\", \"even\", \"even\", \"evidence\", \"evidence\", \"evil\", \"existence\", \"faith\", \"fallacy\", \"false\", \"file\", \"first\", \"first\", \"first\", \"flight\", \"format\", \"ftp\", \"fuel\", \"fund\", \"gay\", \"gif\", \"give\", \"give\", \"give\", \"god\", \"god\", \"good\", \"good\", \"good\", \"graphic\", \"graphics\", \"hi\", \"high\", \"high\", \"high\", \"image\", \"image\", \"image\", \"include\", \"include\", \"include\", \"indeed\", \"info\", \"information\", \"information\", \"information\", \"internet\", \"interpretation\", \"islam\", \"jesus\", \"jpeg\", \"kill\", \"kill\", \"km\", \"know\", \"know\", \"know\", \"koresh\", \"launch\", \"law\", \"law\", \"life\", \"life\", \"like\", \"like\", \"like\", \"line\", \"line\", \"line\", \"look\", \"look\", \"look\", \"love\", \"love\", \"lunar\", \"mac\", \"mail\", \"mail\", \"many\", \"many\", \"many\", \"mar\", \"mar\", \"mar\", \"matthew\", \"matthew\", \"may\", \"may\", \"may\", \"mean\", \"mean\", \"mean\", \"mission\", \"mission\", \"moon\", \"moral\", \"morality\", \"mormon\", \"muslim\", \"nasa\", \"nasa\", \"national\", \"national\", \"new\", \"new\", \"new\", \"nothing\", \"nothing\", \"nothing\", \"op\", \"orbit\", \"orbital\", \"orbiter\", \"package\", \"package\", \"payload\", \"pc\", \"people\", \"people\", \"people\", \"pixel\", \"planetary\", \"please\", \"please\", \"point\", \"point\", \"point\", \"polygon\", \"probe\", \"program\", \"program\", \"propulsion\", \"pub\", \"radio\", \"really\", \"really\", \"really\", \"reason\", \"reason\", \"religion\", \"religious\", \"report\", \"report\", \"report\", \"research\", \"research\", \"research\", \"right\", \"right\", \"right\", \"rocket\", \"russian\", \"satellite\", \"science\", \"science\", \"science\", \"screen\", \"seem\", \"seem\", \"send\", \"send\", \"send\", \"server\", \"service\", \"service\", \"sgi\", \"shuttle\", \"sky\", \"software\", \"software\", \"solar\", \"something\", \"something\", \"space\", \"space\", \"spacecraft\", \"star\", \"star\", \"statement\", \"statement\", \"station\", \"system\", \"system\", \"system\", \"take\", \"take\", \"take\", \"team\", \"technology\", \"technology\", \"thanks\", \"thanks\", \"thing\", \"thing\", \"thing\", \"think\", \"think\", \"think\", \"tiff\", \"time\", \"time\", \"time\", \"true\", \"true\", \"true\", \"truth\", \"truth\", \"unix\", \"us\", \"us\", \"us\", \"use\", \"use\", \"use\", \"user\", \"user\", \"user\", \"vehicle\", \"venus\", \"verse\", \"version\", \"version\", \"version\", \"viewer\", \"way\", \"way\", \"way\", \"well\", \"well\", \"well\", \"window\", \"window\", \"woman\", \"word\", \"word\", \"word\", \"work\", \"work\", \"work\", \"write\", \"write\", \"write\", \"year\", \"year\", \"year\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 1, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el3738818898202022565947463322\", ldavis_el3738818898202022565947463322_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el3738818898202022565947463322\", ldavis_el3738818898202022565947463322_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el3738818898202022565947463322\", ldavis_el3738818898202022565947463322_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=                 x            y  topics  cluster       Freq\n",
       "topic                                                      \n",
       "2     -1015.700562  1338.981201       1        1  48.296311\n",
       "0      -387.996002  -373.316162       2        1  27.138814\n",
       "1     -2184.745605   -60.565800       3        1  24.564875, topic_info=        Term         Freq        Total Category  logprob  loglift\n",
       "846    space  1043.000000  1043.000000  Default  30.0000  30.0000\n",
       "416    image   847.000000   847.000000  Default  29.0000  29.0000\n",
       "329     file   547.000000   547.000000  Default  28.0000  28.0000\n",
       "479   launch   399.000000   399.000000  Default  27.0000  27.0000\n",
       "368      god   784.000000   784.000000  Default  26.0000  26.0000\n",
       "..       ...          ...          ...      ...      ...      ...\n",
       "913     time   194.263809   654.577220   Topic3  -4.9941   0.1891\n",
       "787  science   136.184946   237.633554   Topic3  -5.3493   0.8471\n",
       "700  program   171.648642   533.315724   Topic3  -5.1179   0.2702\n",
       "234   design   122.551919   177.199522   Topic3  -5.4548   1.0351\n",
       "421  include   130.012692   381.757861   Topic3  -5.3957   0.3267\n",
       "\n",
       "[214 rows x 6 columns], token_table=      Topic      Freq       Term\n",
       "term                            \n",
       "0         2  0.993905         3d\n",
       "22        1  0.978019      agree\n",
       "22        3  0.019560      agree\n",
       "24        1  0.011870  algorithm\n",
       "24        2  0.985250  algorithm\n",
       "...     ...       ...        ...\n",
       "993       2  0.470744      write\n",
       "993       3  0.060648      write\n",
       "996       1  0.294098       year\n",
       "996       2  0.020361       year\n",
       "996       3  0.685474       year\n",
       "\n",
       "[314 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 1, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run the visualization [mds is a function to use for visualizing the \"distance\" between topics]\n",
    "pyLDAvis.sklearn.prepare(lda_news_3_topics, bow_news_corpus, bow_vectorizer_news, mds='tsne')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 2.3:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "computer and space are grouped together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How To Find Dominant Topic in a Document\n",
    "\n",
    "Each document typically contains several topics. One of the topics is **dominant**, i.e. it is the largest topic in the document. That topic gives you an answer to the question: **What is this document about?** In other words, the document's dominant topic **summarizes** the document. \n",
    "\n",
    "Let's assign a dominant topic to **each document** in our corpus. Weights in a word vector for a topic provide a measure of association for the word with the topic. If you sum weights for a particular topic across all words in a document, you'll get the weight of that topic in the document.\n",
    "\n",
    "The attribute **.transform** to our function **lda_news** computes the weights of each topic in documents: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_news_topic_weights = lda_news.transform(bow_news_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert lda_news_topic_weights into a nice-looking dataframe and have a look at the computed topic weights in documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_0</th>\n",
       "      <th>Topic_1</th>\n",
       "      <th>Topic_2</th>\n",
       "      <th>Topic_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc_0</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc_1</th>\n",
       "      <td>0.4175</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.5774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc_2</th>\n",
       "      <td>0.0109</td>\n",
       "      <td>0.9678</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc_3</th>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.2117</td>\n",
       "      <td>0.7668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc_4</th>\n",
       "      <td>0.0418</td>\n",
       "      <td>0.0418</td>\n",
       "      <td>0.0421</td>\n",
       "      <td>0.8743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Topic_0  Topic_1  Topic_2  Topic_3\n",
       "Doc_0   0.0005   0.9986   0.0005   0.0005\n",
       "Doc_1   0.4175   0.0026   0.0025   0.5774\n",
       "Doc_2   0.0109   0.9678   0.0107   0.0106\n",
       "Doc_3   0.0107   0.0108   0.2117   0.7668\n",
       "Doc_4   0.0418   0.0418   0.0421   0.8743"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#array of document \"names\" and topic \"names\" (\"names\" are just indecies)\n",
    "doc_names = [\"Doc_\" + str(i) for i in range(len(normalized_corpus_news))]\n",
    "topic_names = [\"Topic_\" + str(i) for i in range(4)]\n",
    "\n",
    "#convert to dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_news_topic_weights, 4), columns=topic_names, index=doc_names)\n",
    "df_document_topic.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that in document Doc_0 the **dominant topic** is Topic_2 as it has the weight of 0.9986. The weights across the 4 topics sum up to 1. Let's add a column that shows dominant topic for each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_0</th>\n",
       "      <th>Topic_1</th>\n",
       "      <th>Topic_2</th>\n",
       "      <th>Topic_3</th>\n",
       "      <th>dominant_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc_0</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc_1</th>\n",
       "      <td>0.4175</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.5774</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc_2</th>\n",
       "      <td>0.0109</td>\n",
       "      <td>0.9678</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc_3</th>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.2117</td>\n",
       "      <td>0.7668</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc_4</th>\n",
       "      <td>0.0418</td>\n",
       "      <td>0.0418</td>\n",
       "      <td>0.0421</td>\n",
       "      <td>0.8743</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Topic_0  Topic_1  Topic_2  Topic_3  dominant_topic\n",
       "Doc_0   0.0005   0.9986   0.0005   0.0005               1\n",
       "Doc_1   0.4175   0.0026   0.0025   0.5774               3\n",
       "Doc_2   0.0109   0.9678   0.0107   0.0106               1\n",
       "Doc_3   0.0107   0.0108   0.2117   0.7668               3\n",
       "Doc_4   0.0418   0.0418   0.0421   0.8743               3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vector of indecies for columns with the highest value by each row in df_document_topic\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "\n",
    "#add dominant_topic as a column to df_document_topic\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "df_document_topic.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Model Evaluation: Log-likelihood, Perplexity and Coherence Scores\n",
    "\n",
    "Log-likelihood, Perplexity and Coherence Score are **measures of performance** for a topic model. They are used for comparing and discriminating between topic models estimated on the same data. Log-likelihood, perplexity and coherence scores **do not have** a baseline or a threshold values and therefore are useful only for comparing models. \n",
    "\n",
    "How do you specify different models? You can set **different number of topics** and also play with the **parameters of the Dirichlet distributions**. \n",
    "\n",
    "#### Coherence Score\n",
    "\n",
    "We will use a function **CoherenceModel()** from the **gensim** module (you can also explore that package as it can be used to estimate an LDA model). The sklearn module does not have the functionality to compute the coherence score. Let's install the gensim package and the functions needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: You are using pip version 21.1; however, version 22.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gensim) (1.6.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gensim) (0.29.23)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gensim) (1.20.2)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install gensim\n",
    "import gensim\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function CoherenceModel() needs as **inputs**:\n",
    "\n",
    "**1. Dictionary of the corpus**<br>\n",
    "**2. Corpus with each document represented as Bag-of-Words**<br>\n",
    "**3. An array of top words for each topic: we'll have top 20 words for each topic** \n",
    "  \n",
    "We will now create those objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing the corpus\n",
    "news_corpus_tokenized = [tokenize_text(normalized_corpus_news[doc_id]) for doc_id in range(len(normalized_corpus_news))]\n",
    "\n",
    "#Dictionary of the corpus:\n",
    "news_dictionary = Dictionary(news_corpus_tokenized)\n",
    "\n",
    "#Bag-of-words representation for each document of the corpus:\n",
    "news_corpus_bow = [news_dictionary.doc2bow(doc) for doc in news_corpus_tokenized]\n",
    "\n",
    "#top 20 words for each topic (using the function defined in session prep)\n",
    "topic_topwords = get_topic_words(vectorizer = bow_vectorizer_news, lda_model = lda_news, n_words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute **the coherence score for the model overall**. We use one of the coherence metrics \"u-mass\" which measures semantic similarity of words in a topic, but there are other metrics as well.\n",
    "\n",
    "*Note: You can check out different coherence metrics here if you are interested: https://dl.acm.org/doi/abs/10.1145/2684822.2685324*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence score for the model:  -1.46\n"
     ]
    }
   ],
   "source": [
    "cm = CoherenceModel(topics=topic_topwords, \n",
    "                    corpus = news_corpus_bow , \n",
    "                    dictionary = news_dictionary, coherence='u_mass')\n",
    "print(\"Coherence score for the model: \", np.round(cm.get_coherence(), 4))  # get coherence value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also see **coherence scores by topic**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence score by topic (higher values are better):  [-1.3002 -1.408  -1.3917 -1.7402]\n"
     ]
    }
   ],
   "source": [
    "print(\"Coherence score by topic (higher values are better): \", np.round(cm.get_coherence_per_topic(),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log-Likelihood Score\n",
    "\n",
    "To compute the log-likelihood score we use the **.score** attribute of our defined and fitted LDA function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Likelihood (higher values are better):  -741050.7113211687\n"
     ]
    }
   ],
   "source": [
    "print(\"Log-Likelihood (higher values are better): \", lda_news.score(bow_news_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perplexity Score\n",
    "\n",
    "To compute the Perplexity score we use the **.perplexity** attribute of our defined and fitted LDA function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity (lower values are better):  573.2690378037004\n"
     ]
    }
   ],
   "source": [
    "print(\"Perplexity (lower values are better): \", lda_news.perplexity(bow_news_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font color=green>EXERCISE 3</font>**\n",
    "\n",
    "**<font color=green>Compare the coherence score, perplexity score and the log-likelihood for models with 2, 3, and 4 topics with your human-judgment-based evaluation of those models. What do you find? </font>**\n",
    "\n",
    "**<font color=green>What you need to do:</font>**\n",
    "\n",
    "**<font color=green>3.1. For model with 4 topics - All code work is done: The model and evaluation metrics are already computed above. You just need to look up the values for the coherence, perplexity and log-likelihood for the model with 4 topics above and discuss what you observe. You might be interested in looking at coherence score by topic as well;</font>**\n",
    "\n",
    "**<font color=green>3.2. For model with 3 topics - The model is computed in Exercise 2. You need to compute the perplexity, log-likelihood and coherence scores for the model with 3 topics (the lines for the coherence score are provided below) and dicuss your results;</font>**\n",
    "\n",
    "**<font color=green>3.3. For model with 2 topics - You need to fit the model with 2 topics and compute all 3 evaluation metrics; dicuss your results.</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 3.1:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion: \n",
    "* Coherence score for the model:  -1.4393; Log-Likelihood (higher values are better):  -740894.0975086405; Perplexity (lower values are better):  572.5000568682434\n",
    "* The performance is not good. Coherence score and Log-Likelihood are better with high value and Log-Likelihood low value better.\n",
    "It have negative for former but positive for later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 3.2:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code (complete the lines):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Likelihood (higher values are better):  -743491.2502467986\n",
      "Perplexity (lower values are better):  585.3865294721026\n",
      "Coherence score for the model: (higher values are better) -1.448\n"
     ]
    }
   ],
   "source": [
    "#Log-Likelihood (add code):\n",
    "print(\"Log-Likelihood (higher values are better): \", lda_news_3_topics.score(bow_news_corpus))\n",
    "\n",
    "#Perplexity score (add code):\n",
    "print(\"Perplexity (lower values are better): \", lda_news_3_topics.perplexity(bow_news_corpus))\n",
    "\n",
    "#Coherence score for 3 topics:\n",
    "topic_topwords_3_topics = get_topic_words(vectorizer = bow_vectorizer_news, lda_model = lda_news_3_topics, n_words=20)\n",
    "cm_3_topics = CoherenceModel(topics=topic_topwords_3_topics, \n",
    "                             corpus = news_corpus_bow, \n",
    "                             dictionary = news_dictionary, coherence='u_mass')\n",
    "\n",
    "#Overall coherence score for the model:\n",
    "print(\"Coherence score for the model: (higher values are better)\", np.round(cm_3_topics.get_coherence(), 3))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "The performance is worse. log-like higher and perplexity lower; coherence is lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 3.3:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Likelihood (higher values are better):  -751870.7915093843\n",
      "Perplexity (lower values are better):  628.9748407328606\n",
      "Coherence score for the model: (higher values are better) -1.42\n"
     ]
    }
   ],
   "source": [
    "#Fit LDA with 2 topics:\n",
    "lda_news_2_topics = LatentDirichletAllocation(n_components=2, max_iter=100,\n",
    "                                     doc_topic_prior = 0.25,\n",
    "                                     topic_word_prior = 0.25).fit(bow_news_corpus)\n",
    "\n",
    "\n",
    "#Log-Likelihood:\n",
    "print(\"Log-Likelihood (higher values are better): \", lda_news_2_topics.score(bow_news_corpus))\n",
    "\n",
    "#Perplexity score:\n",
    "print(\"Perplexity (lower values are better): \", lda_news_2_topics.perplexity(bow_news_corpus))\n",
    "\n",
    "#Coherence score for 3 topics:\n",
    "topic_topwords_2_topics = get_topic_words(vectorizer = bow_vectorizer_news, lda_model = lda_news_2_topics, n_words=20)\n",
    "cm_2_topics = CoherenceModel(topics=topic_topwords_2_topics, \n",
    "                             corpus = news_corpus_bow, \n",
    "                             dictionary = news_dictionary, coherence='u_mass')\n",
    "\n",
    "#Overall coherence score for the model:\n",
    "\n",
    "print(\"Coherence score for the model: (higher values are better)\", np.round(cm_2_topics.get_coherence(), 2))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "*log-like lower, perplexity higher, coherence lower than 3-topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overall discussion for EXERCISE 3**:\n",
    "* 4-topic did better job than 3 topic, 3 topic better than 2 topic. It is consistant with the truth we know that:\n",
    "* The newspaper blog posts have 4 topics: atheism, religion, computer graphics, and space science\n",
    "* It make sense that 4 topic has the best result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>**NOTE:** Generally, you can write a simple script that selects the best topic model **automatically** based on a criterion for \"best model\" (log-likelihood, perplexity, or coherence score). The script can vary both parameters of the Dirichlet distributions and the number of topics, or just the number of topics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
