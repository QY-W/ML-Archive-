{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: TEXT NORMALIZATION and VECTORIZATION <br>\n",
    "\n",
    "\n",
    "**<font color=green>INSTRUCTIONS:</font>** <br> <br>\n",
    "    **<font color=green>1. Look for EXERCISES and QUESTIONS in this script. </font>** <br> <br>\n",
    "    **<font color=green>2. Each student INDIVIDUALLY uploads this script with their answers embedded (and other materials if requested) to Canvas by the the deadline indicated on Canvas.</font>** <br>\n",
    "## SESSION PREP\n",
    "\n",
    "### How to install any module from inside Jupyter\n",
    "\n",
    "To be able to install any module from inside Jupyper, we need module called sys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can install any module from Jupyter by running a line such as: <br> <br> !{sys.executable} -m pip install module_name\n",
    "\n",
    "### Install Natural Language ToolKit (NLTK) module (and some other modules)\n",
    "\n",
    "The NLTK module does text normalization, among other functions. We'll install module NLTK, as well as modules numpy and pandas, from inside Jupyter (you might see deprication warnings in pink about future changes in the module but you do not need to pay attention to them at this time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: You are using pip version 21.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.6.5)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (2021.11.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (4.62.2)\n",
      "Requirement already satisfied: click in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.20.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: You are using pip version 21.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.2.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas) (1.20.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: You are using pip version 21.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install nltk\n",
    "import nltk\n",
    "\n",
    "!{sys.executable} -m pip install numpy\n",
    "import numpy as np \n",
    "\n",
    "!{sys.executable} -m pip install pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download text data\n",
    "\n",
    "In what follows, we'll use an electronic archive of books from Project Gutenberg that Natural Language ToolKit has access to. In particular, we'll use \"Alice in Wonderland\" by Lewis Carrol. Our corpus will be just one file called carroll-alice.txt (it's in .txt format):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"[Alice's Adventures in Wonderland b\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Willi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('gutenberg') \n",
    "from nltk.corpus import gutenberg \n",
    "\n",
    "alice = gutenberg.raw(fileids='carroll-alice.txt') # we name the corpus 'alice'\n",
    "from pprint import pprint #function for pretty printing\n",
    "pprint(alice[0:35]) #print the first 35 characters of the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEXT TOKENIZATION\n",
    "**Tokenization** is splitting text into sematically meaningful chuncks, such as sentences or words. Tokenizing into words is most common. You might be interested in tokenizing into sentences if you plan to analyze text sentence by sentence.\n",
    "\n",
    "### Tokenization by Sentence\n",
    "From the NLTK module, we'll use a sentence tokenizer 'punkt':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Willi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now tokenize the Alice corpus by sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total sentences in the corpus: 1625\n"
     ]
    }
   ],
   "source": [
    "alice_sentences = nltk.sent_tokenize(text=alice)\n",
    "print('\\nTotal sentences in the corpus:', len(alice_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the first sentence in the Alice corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First sentence in alice: [Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I.\n"
     ]
    }
   ],
   "source": [
    "print('\\nFirst sentence in alice:', alice_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at what the second sentence looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Second sentence in alice: Down the Rabbit-Hole\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do: once or twice she had peeped into the\n",
      "book her sister was reading, but it had no pictures or conversations in\n",
      "it, 'and what is the use of a book,' thought Alice 'without pictures or\n",
      "conversation?'\n"
     ]
    }
   ],
   "source": [
    "print('\\nSecond sentence in alice:', alice_sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>QUESTION 1: Why do you think the first and second tokenized sentences (above) look like that? (look at what Python printed out)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Answer (double-click this cell to write here):\n",
    "- There are a lot of meaningless words like was and to, and -ing punctuation and -s does not help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization into Words\n",
    "Let's do some tokenization into words now. You can tokenize into words using punctuation signs, white spaces, or \"words\".\n",
    "\n",
    "We'll tokenize a corpus consisting of one sentence shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The brown fox wasn't that quick and he couldn't win the races\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize **using \"words\"**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'races']\n"
     ]
    }
   ],
   "source": [
    "words = nltk.word_tokenize(sentence)\n",
    "print(words)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize **using punctuation signs** now. Do you see any difference between this tokenization and the previous one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'wasn', \"'\", 't', 'that', 'quick', 'and', 'he', 'couldn', \"'\", 't', 'win', 'the', 'races']\n"
     ]
    }
   ],
   "source": [
    "wordpunkt_wt = nltk.WordPunctTokenizer()\n",
    "words = wordpunkt_wt.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize **using white spaces**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'races']\n"
     ]
    }
   ],
   "source": [
    "whitespace_wt = nltk.WhitespaceTokenizer()\n",
    "words = whitespace_wt.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get rid of stopwords (\"it's\", \"is\", \"the\", etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'haven', 'an', 'before', 'its', \"it's\", 'that', 't', \"don't\", 'than', 'aren', 'out', 've', 'few', 'while', \"hadn't\", 'so', 'll', 'very', 'them', 'what', 'couldn', 'itself', 'when', \"didn't\", 'theirs', 'don', 'should', 'are', 'each', 'other', \"haven't\", 'from', 'and', 'but', 'off', 'between', 'hasn', 'under', 'can', 'in', \"should've\", 'was', 'this', 'which', \"needn't\", 'of', 'o', 'shan', 'who', 'during', 'both', \"shan't\", \"you're\", 'been', 'against', \"wasn't\", 'on', 'me', 'they', 'where', 'further', 'do', \"doesn't\", 'am', 'at', 'those', \"shouldn't\", 'doesn', 'we', 'through', 'ma', 'any', 'their', 'has', 'does', 'you', 'all', 'having', 'no', 'here', 'now', 'until', 'himself', 'whom', \"won't\", 'is', 'myself', 'these', 'to', \"wouldn't\", 'once', 'just', 'our', 'had', 'only', 'up', 'about', 'mustn', 'yours', 'such', 'nor', 's', 'as', 'she', 'or', 'again', 'not', 'why', 'y', 'being', 'will', 'the', 'below', 'over', 'shouldn', 'ours', 'it', 'themselves', 'because', 'into', 'too', 're', \"weren't\", 'i', 'weren', 'doing', \"you'll\", 'her', \"you'd\", 'herself', 'down', \"mightn't\", 'most', 'yourselves', \"hasn't\", 'my', 'yourself', 'your', 'a', 'own', 'didn', 'wasn', 'some', 'be', 'have', 'by', 'ain', 'he', \"aren't\", 'm', 'more', 'hadn', 'were', 'how', \"you've\", 'wouldn', 'then', 'isn', 'won', 'above', 'him', 'ourselves', \"couldn't\", 'if', \"mustn't\", 'with', 'after', 'same', 'hers', 'needn', 'mightn', \"she's\", 'd', \"that'll\", 'did', 'for', 'there', 'his', \"isn't\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Willi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can (and should consider) amending the list of stopwords given your data and project objectives. For example, we can add more stopwords to the standard list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'haven', 'an', 'before', 'its', \"it's\", 'that', 't', \"don't\", 'than', 'aren', 'out', 've', 'few', 'while', \"hadn't\", 'so', 'll', 'very', 'them', 'what', 'couldn', 'itself', 'when', \"didn't\", 'theirs', 'don', 'should', 'are', 'each', 'other', \"haven't\", 'from', 'and', 'but', 'off', 'between', 'hasn', 'under', 'can', 'in', \"should've\", 'was', 'this', 'which', \"needn't\", 'of', 'o', 'who', 'during', 'both', \"shan't\", \"you're\", 'been', 'against', \"wasn't\", 'on', 'me', 'they', 'where', \"isn't\", 'his', 'further', 'do', \"doesn't\", 'am', 'at', 'those', \"shouldn't\", 'doesn', 'we', 'through', 'ma', 'any', 'their', 'has', 'does', 'you', 'all', 'having', 'no', 'here', 'now', 'until', 'himself', 'whom', \"won't\", 'is', 'myself', 'these', 'to', \"wouldn't\", 'once', 'just', 'our', 'had', 'only', 'up', 'about', 'mustn', 'yours', 'such', 'nor', 's', 'as', 'she', 'or', 'again', 'not', 'why', 'y', 'being', 'will', 'the', 'below', 'over', 'shouldn', 'ours', 'it', 'themselves', 'because', 'into', 'too', 're', \"weren't\", 'i', 'weren', 'doing', \"you'll\", 'her', \"you'd\", 'herself', 'down', \"mightn't\", 'most', 'yourselves', \"hasn't\", 'my', 'yourself', 'your', 'a', 'own', 'didn', 'wasn', 'some', 'be', 'have', 'by', 'ain', \"aren't\", 'm', 'more', 'hadn', 'were', 'how', \"you've\", 'wouldn', 'then', 'isn', 'won', 'above', 'him', 'ourselves', \"couldn't\", 'if', \"mustn't\", 'with', 'after', 'same', 'hers', 'needn', 'mightn', \"she's\", 'd', \"that'll\", 'did', 'for', 'there', 'he', 'shan', 'NYC'}\n"
     ]
    }
   ],
   "source": [
    "add_stopwords ={'so','NYC'}\n",
    "stop_words_new = add_stopwords.union(stop_words)\n",
    "print(stop_words_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compare the tokenized sentence before and after removing the stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'races']\n",
      "Filterd Sentence (without stopwords): ['The', 'brown', 'fox', 'quick', 'win', 'races']\n"
     ]
    }
   ],
   "source": [
    "filtered_tokens=[]\n",
    "\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filtered_tokens.append(w)\n",
    "        \n",
    "print(\"Tokenized Sentence:\",words)\n",
    "print(\"Filterd Sentence (without stopwords):\",filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEMMING AND LEMMATIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's stem the sentence first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Sentence: ['The', 'brown', 'fox', 'quick', 'win', 'races']\n",
      "Stemmed Sentence: ['the', 'brown', 'fox', 'quick', 'win', 'race']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stemmed_tokens=[]\n",
    "for w in filtered_tokens:\n",
    "    stemmed_tokens.append(ps.stem(w))\n",
    "\n",
    "print(\"Filtered Sentence:\",filtered_tokens)\n",
    "print(\"Stemmed Sentence:\",stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare stemming to lemmatization for the word \"running\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Willi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Word: run\n",
      "Stemmed Word: run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "word = \"running\"\n",
    "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\")) # 'v' indicates that the word is a verb\n",
    "print(\"Stemmed Word:\",ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more comparison for the word \"bought\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Word: buy\n",
      "Stemmed Word: bought\n"
     ]
    }
   ],
   "source": [
    "word = \"bought\"\n",
    "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\")) # 'v' indicates that the word is a verb (part-of-speech)\n",
    "print(\"Stemmed Word:\",ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>EXERCISE 1: What result would you get if you change the part-of-speech tag in the lemmatization line above to \"n\", which means \"noun\"? (look at what Python printed out)</font> <br>\n",
    "Your Answer in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Word: bought\n"
     ]
    }
   ],
   "source": [
    "print(\"Lemmatized Word:\",lem.lemmatize(word,\"n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VECTORIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text vectorization is the process of feature extraction from text data, that is the process of creating variables for each observation, where an observation is a text document. We'll consider the **bag-of-words**, the **TF-IDF** and the **n-grams** vectorized representations of text. <br>\n",
    "\n",
    "Let's vectorize the corpus about \"blue skies and blue cheese\" similar to one used in the video lecture: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['the sky is blue',\n",
    "          'sky is blue and sky is beautiful', \n",
    "          'the beautiful sky is so blue',\n",
    "          'i love blue cheese']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use built-in vectorizers from Scikit-Learn module for machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words Representation\n",
    "\n",
    "We'll use bag-of-words representation (CountVectorizer) first. You can see the documentation here:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sklearn) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->sklearn) (1.20.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->sklearn) (1.6.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->sklearn) (3.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->sklearn) (1.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: You are using pip version 21.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install sklearn\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is convinient to \"define\" a vectorizer first before applying it. You can specify all the parameters (arguments) of the function in the definition. For example, the max_features parameter below drops all features except for the selected number of most frequent terms in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_BOW = CountVectorizer(max_features=1000) #BOW = bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's extract features using the vectorizer function. Note the .fit_transform function below. It creates the dictionary of the corpus and does the vectorization: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8\n",
       "0  0  0  1  0  1  0  1  0  1\n",
       "1  1  1  1  0  2  0  2  0  0\n",
       "2  0  1  1  0  1  0  1  1  1\n",
       "3  0  0  1  1  0  1  0  0  0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOW_matrix = vectorizer_BOW.fit_transform(corpus).toarray()\n",
    "pd.DataFrame(np.round(BOW_matrix,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to attach the names of the features, right? Here are the names of the features from the dictionary of the corpus (note the function get_feature_names()):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['and', 'beautiful', 'blue', 'cheese', 'is', 'love', 'sky', 'so', 'the']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_BOW.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a more useful looking bag-of-words representation, with feature names attached:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>blue</th>\n",
       "      <th>cheese</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>sky</th>\n",
       "      <th>so</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  beautiful  blue  cheese  is  love  sky  so  the\n",
       "0    0          0     1       0   1     0    1   0    1\n",
       "1    1          1     1       0   2     0    2   0    0\n",
       "2    0          1     1       0   1     0    1   1    1\n",
       "3    0          0     1       1   0     1    0   0    0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.round(BOW_matrix,2),columns=vectorizer_BOW.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization Using N-grams\n",
    "<br>\n",
    "Let's use bi-grams in our vectorized representation of text. First, we define the vectorizer (we need the same CountVectorizer() function) using a parameter for specifying n-grams. Then we apply it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and sky</th>\n",
       "      <th>beautiful sky</th>\n",
       "      <th>blue and</th>\n",
       "      <th>blue cheese</th>\n",
       "      <th>is beautiful</th>\n",
       "      <th>is blue</th>\n",
       "      <th>is so</th>\n",
       "      <th>love blue</th>\n",
       "      <th>sky is</th>\n",
       "      <th>so blue</th>\n",
       "      <th>the beautiful</th>\n",
       "      <th>the sky</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and sky  beautiful sky  blue and  blue cheese  is beautiful  is blue  \\\n",
       "0        0              0         0            0             0        1   \n",
       "1        1              0         1            0             1        1   \n",
       "2        0              1         0            0             0        0   \n",
       "3        0              0         0            1             0        0   \n",
       "\n",
       "   is so  love blue  sky is  so blue  the beautiful  the sky  \n",
       "0      0          0       1        0              0        1  \n",
       "1      0          0       2        0              0        0  \n",
       "2      1          0       1        1              1        0  \n",
       "3      0          1       0        0              0        0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_Bi_Grams = CountVectorizer(max_features=1000, ngram_range=(2, 2))\n",
    "Bi_Grams_matrix = vectorizer_Bi_Grams.fit_transform(corpus).toarray()\n",
    "pd.DataFrame(np.round(Bi_Grams_matrix,2),columns=vectorizer_Bi_Grams.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>EXERCISE 2: Create a Bi-Grams vectorizer that uses the mix of bi-grams and uni-grams. To complete the Exercise you may need to look up CountVectorizer's documentation, see link below.</font> <br>\n",
    "\n",
    "Documentation: \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html <br>\n",
    "\n",
    "Your Answer in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>and sky</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>beautiful sky</th>\n",
       "      <th>blue</th>\n",
       "      <th>blue and</th>\n",
       "      <th>blue cheese</th>\n",
       "      <th>cheese</th>\n",
       "      <th>is</th>\n",
       "      <th>is beautiful</th>\n",
       "      <th>...</th>\n",
       "      <th>is so</th>\n",
       "      <th>love</th>\n",
       "      <th>love blue</th>\n",
       "      <th>sky</th>\n",
       "      <th>sky is</th>\n",
       "      <th>so</th>\n",
       "      <th>so blue</th>\n",
       "      <th>the</th>\n",
       "      <th>the beautiful</th>\n",
       "      <th>the sky</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  and sky  beautiful  beautiful sky  blue  blue and  blue cheese  \\\n",
       "0    0        0          0              0     1         0            0   \n",
       "1    1        1          1              0     1         1            0   \n",
       "2    0        0          1              1     1         0            0   \n",
       "3    0        0          0              0     1         0            1   \n",
       "\n",
       "   cheese  is  is beautiful  ...  is so  love  love blue  sky  sky is  so  \\\n",
       "0       0   1             0  ...      0     0          0    1       1   0   \n",
       "1       0   2             1  ...      0     0          0    2       2   0   \n",
       "2       0   1             0  ...      1     0          0    1       1   1   \n",
       "3       1   0             0  ...      0     1          1    0       0   0   \n",
       "\n",
       "   so blue  the  the beautiful  the sky  \n",
       "0        0    1              0        1  \n",
       "1        0    0              0        0  \n",
       "2        1    1              1        0  \n",
       "3        0    0              0        0  \n",
       "\n",
       "[4 rows x 21 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_Bi_Grams = CountVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "Bi_Grams_matrix = vectorizer_Bi_Grams.fit_transform(corpus).toarray()\n",
    "pd.DataFrame(np.round(Bi_Grams_matrix,2),columns=vectorizer_Bi_Grams.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization with Term Frequency – Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "Now, let's do feature extraction (vectorization) using the TF-IDF approach. <br> <br> See full documentation here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer <br> <br>\n",
    "Import the vectorizer first and define it by specifying the functions (look up the specified parameters in the documentation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "vectorizer_TF_IDF = TfidfVectorizer(norm = None, smooth_idf = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's vectorize our corpus now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>blue</th>\n",
       "      <th>cheese</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>sky</th>\n",
       "      <th>so</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.92</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.22</td>\n",
       "      <td>1.92</td>\n",
       "      <td>1.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
       "0  0.00       0.00   1.0    0.00  1.22  0.00  1.22  0.00  1.51\n",
       "1  1.92       1.51   1.0    0.00  2.45  0.00  2.45  0.00  0.00\n",
       "2  0.00       1.51   1.0    0.00  1.22  0.00  1.22  1.92  1.51\n",
       "3  0.00       0.00   1.0    1.92  0.00  1.92  0.00  0.00  0.00"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF_IDF_matrix = vectorizer_TF_IDF.fit_transform(corpus).toarray()\n",
    "pd.DataFrame(np.round(TF_IDF_matrix, 2), columns=vectorizer_TF_IDF.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the IDF weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.92 1.51 1.   1.92 1.22 1.92 1.22 1.92 1.51]\n"
     ]
    }
   ],
   "source": [
    "print(np.round(vectorizer_TF_IDF.idf_,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a good idea to normalize the TF-IDF matrix, i.e. restrict all entries to be between 0 and 1. Some text mining models require normalized matrices. Norm parameter is used for this purpose (you can look it up in the documentation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>blue</th>\n",
       "      <th>cheese</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>sky</th>\n",
       "      <th>so</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
       "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n",
       "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n",
       "2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n",
       "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_TF_IDF = TfidfVectorizer(norm = 'l2', smooth_idf = True)\n",
    "TF_IDF_matrix = vectorizer_TF_IDF.fit_transform(corpus).todense()\n",
    "pd.DataFrame(np.round(TF_IDF_matrix,2), columns=vectorizer_TF_IDF.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font color=green> EXERCISE 3: You are given a new small corpus called corpus_exercise (see below). Your ultimate task is to normalize (pre-process) the corpus and produce the TF-IDF and the Bag-of-Words representations of the data. Follow the steps below to complete this exercise:</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. Download a file Text_Normalization_Function.ipynb from Canvas and put it into the same directory(!) as the current Jupyter notebook. That file defines a relatively sophisticated text normalization function. (OPTIONAL: you can explore what that file does when you are done with this exercise.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. Run the file Text_Normalization_Function.ipynb to define the text normalization function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: html.parser in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.2)\n",
      "Requirement already satisfied: ply in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from html.parser) (3.11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: You are using pip version 21.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pattern3 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pattern3) (4.10.0)\n",
      "Requirement already satisfied: cherrypy in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pattern3) (18.6.1)\n",
      "Requirement already satisfied: docx in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pattern3) (0.2.4)\n",
      "Requirement already satisfied: feedparser in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pattern3) (6.0.8)\n",
      "Requirement already satisfied: pdfminer3k in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pattern3) (1.3.4)\n",
      "Requirement already satisfied: simplejson in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pattern3) (3.17.5)\n",
      "Requirement already satisfied: pdfminer.six in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pattern3) (20211012)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from beautifulsoup4->pattern3) (2.2.1)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cherrypy->pattern3) (8.10.0)\n",
      "Requirement already satisfied: jaraco.collections in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cherrypy->pattern3) (3.4.0)\n",
      "Requirement already satisfied: pywin32>=227 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cherrypy->pattern3) (300)\n",
      "Requirement already satisfied: zc.lockfile in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cherrypy->pattern3) (2.0)\n",
      "Requirement already satisfied: cheroot>=8.2.1 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cherrypy->pattern3) (8.5.2)\n",
      "Requirement already satisfied: portend>=2.1.1 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cherrypy->pattern3) (3.0.0)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cheroot>=8.2.1->cherrypy->pattern3) (1.15.0)\n",
      "Requirement already satisfied: jaraco.functools in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cheroot>=8.2.1->cherrypy->pattern3) (3.4.0)\n",
      "Requirement already satisfied: tempora>=1.8 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from portend>=2.1.1->cherrypy->pattern3) (4.1.2)\n",
      "Requirement already satisfied: pytz in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern3) (2021.1)\n",
      "Requirement already satisfied: lxml in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from docx->pattern3) (4.6.4)\n",
      "Requirement already satisfied: Pillow>=2.0 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from docx->pattern3) (8.3.2)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from feedparser->pattern3) (1.0.0)\n",
      "Requirement already satisfied: jaraco.text in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from jaraco.collections->cherrypy->pattern3) (3.6.0)\n",
      "Requirement already satisfied: jaraco.classes in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from jaraco.collections->cherrypy->pattern3) (3.2.1)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from jaraco.text->jaraco.collections->cherrypy->pattern3) (5.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from importlib-resources->jaraco.text->jaraco.collections->cherrypy->pattern3) (3.6.0)\n",
      "Requirement already satisfied: chardet in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pdfminer.six->pattern3) (4.0.0)\n",
      "Requirement already satisfied: cryptography in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pdfminer.six->pattern3) (35.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cryptography->pdfminer.six->pattern3) (1.14.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cffi>=1.12->cryptography->pdfminer.six->pattern3) (2.20)\n",
      "Requirement already satisfied: ply in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pdfminer3k->pattern3) (3.11)\n",
      "Requirement already satisfied: setuptools in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from zc.lockfile->cherrypy->pattern3) (49.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: You are using pip version 21.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.3.1)\n",
      "Requirement already satisfied: gensim in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis) (4.1.2)\n",
      "Requirement already satisfied: numexpr in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis) (2.7.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis) (1.6.3)\n",
      "Requirement already satisfied: funcy in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis) (1.16)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis) (1.2.4)\n",
      "Requirement already satisfied: future in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis) (1.0.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis) (49.2.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis) (1.20.2)\n",
      "Requirement already satisfied: sklearn in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis) (0.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis) (2.11.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyLDAvis) (1.1.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gensim->pyLDAvis) (0.29.23)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from jinja2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->pyLDAvis) (3.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\\UNKNOWN\n",
      "sysconfig: c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\Include\n",
      "WARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\n",
      "WARNING: You are using pip version 21.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  ['<', 'p', '>', 'The', 'circus', 'dog', 'in', 'a', 'plissé', 'skirt', 'jumped', 'over', 'Python', 'who', 'was', \"n't\", 'that', 'large', ',', 'just', '3', 'feet', 'long.', '<', '/p', '>']\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  <p>The circus dog in a plissé skirt jumped over Python who was not that large, just 3 feet long.</p>\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  [('<', 'a'), ('p', 'n'), ('>', 'v'), ('the', None), ('circus', 'n'), ('dog', 'n'), ('in', None), ('a', None), ('plissé', 'n'), ('skirt', 'n'), ('jumped', 'v'), ('over', None), ('python', 'n'), ('who', None), ('was', 'v'), (\"n't\", 'r'), ('that', None), ('large', 'a'), (',', None), ('just', 'r'), ('3', None), ('feet', 'n'), ('long.', 'a'), ('<', 'n'), ('/p', 'n'), ('>', 'n')]\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  < p > the circus dog in a plissé skirt jump over python who be n't that large , just 3 foot long. < /p >\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:    p   The circus dog in a plissé skirt jumped over Python who was n t that large   just 3 feet long     p  \n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  < p > The circus dog plissé skirt jumped Python n't large , 3 feet long. < /p >\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  p The circus dog in a plissé skirt jumped over Python who was n't that large just feet long. /p\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  <p>The circus dog in a plisse skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Willi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Willi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Willi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Willi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%run ./Text_Normalization_Function.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. Define the corpus_exercise text corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "corpus_exercise = ['python is great for text mining',\n",
    "          'anyone can learn python and do text mining', \n",
    "          'python can go without eating for days',\n",
    "          'python can be a great pet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4. Normalize the corpus_exercise text corpus and call its normalized version NORM_corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['python great text mining',\n",
       " 'anyone learn python text mining',\n",
       " 'python without eat day',\n",
       " 'python great pet']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NORM_corpus = normalize_corpus(corpus_exercise)\n",
    "NORM_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5. Compute and print out the TF-IDF and the Bag-of-Words representations for NORM_corpus (WRITE the lines of code needed in the cell below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   anyone   day   eat  great  learn  mining   pet  python  text  without\n",
      "0    0.00  0.00  0.00   0.54   0.00    0.54  0.00    0.36  0.54     0.00\n",
      "1    0.53  0.00  0.00   0.00   0.53    0.42  0.00    0.28  0.42     0.00\n",
      "2    0.00  0.55  0.55   0.00   0.00    0.00  0.00    0.29  0.00     0.55\n",
      "3    0.00  0.00  0.00   0.57   0.00    0.00  0.73    0.38  0.00     0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\willi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['anyone',\n",
       " 'day',\n",
       " 'eat',\n",
       " 'great',\n",
       " 'learn',\n",
       " 'mining',\n",
       " 'pet',\n",
       " 'python',\n",
       " 'text',\n",
       " 'without']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF\n",
    "vectorizer_TF_IDF = TfidfVectorizer(norm = 'l2', smooth_idf = True)\n",
    "TF_IDF_matrix = vectorizer_TF_IDF.fit_transform(NORM_corpus).todense()\n",
    "print(pd.DataFrame(np.round(TF_IDF_matrix,2), columns=vectorizer_TF_IDF.get_feature_names()))\n",
    "\n",
    "# bag of words\n",
    "BOW_matrix = vectorizer_BOW.fit_transform(NORM_corpus).toarray()\n",
    "pd.DataFrame(np.round(BOW_matrix,2))\n",
    "vectorizer_BOW.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font color=green> OPTIONAL EXERCISE 4: Explore the Text_Normalization_Function.ipynb notebook that defines a text normalization function. The file is available on Canvas in the Lab 2 Assignment (no answer is needed). </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
